{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ab96ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yk2047/Documents/GitHub/hathor/langchein\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment & path setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Make sure \"src\" is importable\n",
    "PROJECT_ROOT = os.path.abspath(\".\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21edc559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary file: /Users/yk2047/Documents/GitHub/hathor/dataset_examples/halo_3517_gas.bin\n",
      "Description file: ./plotting_codes/Example.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import agents, config, utils, pipeline\n",
    "\n",
    "from src.agents import (\n",
    "    LiteratureAgent,\n",
    "    CriticAgent,\n",
    "    ReasonerAgent,\n",
    "    CoderAgent,\n",
    "    RunnerAgent,\n",
    ")\n",
    "from src.pipeline.multi_agent_pipeline import MultiAgentPipeline\n",
    "from src.config import (\n",
    "    LITERATURE_MODEL,\n",
    "    CRITIC_LITERATURE_MODEL,\n",
    "    REASONER_MODEL,\n",
    "    CRITIC_MODEL,\n",
    "    CODER_MODEL,\n",
    "    BINARY_FILE_PATH,\n",
    "    IDEAS_DIR,\n",
    "    PREVIOUS_CODES_DIR,\n",
    "    OUTPUT_FIG_PATH,\n",
    "    FINAL_CODE_PATH,\n",
    "    DATA_DESCRIPTION_PATH,\n",
    ")\n",
    "from src.utils import (\n",
    "    read_codes_from_folder,\n",
    "    save_answer_to_file,\n",
    "    save_code_to_file,\n",
    ")\n",
    "\n",
    "print(\"Binary file:\", BINARY_FILE_PATH)\n",
    "print(\"Description file:\", DATA_DESCRIPTION_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240bfa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents instantiated âœ…\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Instantiate agents (same as pipeline __init__)\n",
    "\n",
    "literature_agent = LiteratureAgent(LITERATURE_MODEL)\n",
    "critic_agent = CriticAgent(CRITIC_LITERATURE_MODEL)\n",
    "reasoner_agent = ReasonerAgent(REASONER_MODEL)\n",
    "coder_agent = CoderAgent(CODER_MODEL)\n",
    "runner_agent = RunnerAgent(coder_agent)\n",
    "\n",
    "print(\"Agents instantiated âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef151cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hypotheses ===\n",
      "**Hypothesisâ€¯1 â€“ Radial â€œcoolâ€‘coreâ€ imprint in the projected electronâ€‘density map**  \n",
      "*What to look for:* When theâ€¯neâ€¯field is summed (or columnâ€‘averaged) along theâ€¯zâ€‘axis, the resulting 2â€‘D image should display a steep central excess of surfaceâ€¯electronâ€‘density that falls off roughly as a Î²â€‘model (â€¯Î£â‚‘(r)â€¯âˆâ€¯[1+(r/râ‚š)Â²]^{â€“3Î²+0.5}â€¯). In a genuine coolâ€‘core cluster this central peak will be sharply confined (râ‚šâ€¯â‰²â€¯100â€¯kpc) and roughly circular, whereas a nonâ€‘coolâ€‘core system will show a flatter core and more irregular isodensity contours. By fitting concentric annuli to the projected map and extracting a radial profile, you can test whether the core slope, core radius, and Î²â€‘parameter match the coolâ€‘core expectations and how they vary with halo mass or redshift within the MEGATRON sample.\n",
      "\n",
      "**Hypothesisâ€¯2 â€“ Gasâ€‘clumping factor rises toward the virial boundary**  \n",
      "*What to look for:* The â€œclumping factorâ€ Câ€¯â‰¡â€¯âŸ¨nâ‚‘Â²âŸ©/âŸ¨nâ‚‘âŸ©Â² can be approximated from the 2â€‘D projection by comparing the pixelâ€‘wise squaredâ€‘density map (Î£â‚‘Â²) with the square of the density map (Î£â‚‘)Â². If clumping is significant, highâ€‘C regions will appear as bright, filamentary spikes or small isolated blobs superimposed on the smoother halo outskirts. By constructing the map of C(r)â€¯=â€¯âŸ¨Î£â‚‘Â²âŸ©_Î¸â€¯/â€¯âŸ¨Î£â‚‘âŸ©_Î¸Â² in concentric radial bins (Î¸ denotes azimuthal averaging), you can test whether C rises from â‰ˆâ€¯1 in the innerâ€¯âˆ¼â€¯0.5â€¯Râ‚â‚‚â‚€â‚€â‚ to â‰³â€¯2â€“3 nearâ€¯Râ‚â‚‚â‚€â‚€â‚, as predicted by cosmologicalâ€‘hydro simulations. The presence (or absence) of a systematic radial upturn would support (or challenge) current models of ICM inhomogeneity.\n",
      "\n",
      "**Hypothesisâ€¯3 â€“ Alignment between highâ€‘refinement (level) structures and density subâ€‘haloes indicates recent mergers**  \n",
      "*What to look for:* The â€œlevelâ€ field encodes the AMR refinement hierarchy; cells at higher levels are typically placed where the gas density or gradients are largest. By projecting the level field (or a weighted combination, e.g., levelâ€¯Ã—â€¯ne) you can expose the spatial distribution of the most refined zones. If a halo has undergone a recent major merger, the highâ€‘level regions will trace elongated, offâ€‘center bridges or â€œcoldâ€‘frontâ€â€“like edges that are not concentric with the main density peak. Conversely, a relaxed halo should show a roughly circular highâ€‘level shell surrounding the central peak. Quantify the hypothesis by measuring the azimuthal variance of the highâ€‘level mask as a function of radius and comparing it to the variance of the projected ne map; a strong correlation (highâ€‘level varianceâ€¯â‰«â€¯ne variance) would indicate mergerâ€‘driven substructure.\n",
      "\n",
      "*Each of these hypotheses can be examined directly with the 2â€‘D projected maps you will generate from the binary cutout:*  \n",
      "\n",
      "- **Radial profiling** (core vs. outskirts) â†’ test the presence of a cool core.  \n",
      "- **Pixelâ€‘wise ratio maps** (Î£â‚‘Â²â€¯/â€¯Î£â‚‘Â²) â†’ quantify clumping as a function of radius.  \n",
      "- **Levelâ€‘field morphology** (highâ€‘level masks) â†’ diagnose dynamical state and recent mergers.  \n",
      "\n",
      "By comparing the observed patterns across multiple halos (different masses, redshifts, or simulation runs) you can assess how universal these signatures are and how they relate to the underlying physics of galaxyâ€‘cluster formation.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test literature_agent.generate_hypotheses()\n",
    "\n",
    "literature_task = \"\"\"\n",
    "I want to do plots for MEGATRON cutout data of gas cells in a halo. It's stored in a binary file. \n",
    "The data contains positions (x,y,z), levels, ne (electron number density), dx (cell size), and other \n",
    "features for each gas cell.\n",
    "\n",
    "I want to create 2D images of parameters projected along the z-axis to learn something interesting \n",
    "about galaxy clusters.\n",
    "\n",
    "Please propose interesting hypotheses about galaxy clusters that can be tested through visualization \n",
    "and analysis of this data.\n",
    "\"\"\"\n",
    "\n",
    "num_hypotheses = 3\n",
    "\n",
    "hypotheses = literature_agent.generate_hypotheses(\n",
    "    task=literature_task,\n",
    "    num_hypotheses=num_hypotheses,\n",
    ")\n",
    "\n",
    "print(\"=== Hypotheses ===\")\n",
    "print(hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8a59fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Refined hypotheses ===\n",
      "Below is a critical appraisal of the three ideas that were put forward, followed by a trimmedâ€‘down set of **realistic, testable hypotheses** that can be answered directly with the 2â€‘D maps you will generate from the Megatron cutâ€‘outs.  I have kept the spirit of each original claim, but removed the parts that are either impossible to measure reliably or that would give a misleading answer, and I have added the quantitative diagnostics that make the tests concrete.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  Radial â€œcoolâ€‘coreâ€ imprint in the projected electronâ€‘density map  \n",
      "\n",
      "### What is problematic?  \n",
      "\n",
      "| Issue | Why it matters | Remedy |\n",
      "|------|----------------|--------|\n",
      "| **Projectionâ€‘induced dilution** â€“ a steep 3â€‘D core becomes much shallower once integrated along a 2â€‘Mpc line of sight. | A Î²â€‘model fit to the *projected* profile will systematically underestimate the true central slope and core radius, especially for halos that are not perfectly spherical. | Use **deâ€‘projected** profiles (Abel inversion or onionâ€‘peeling) on the *cylindrically* averaged surfaceâ€‘density map, or restrict the analysis to the innermost 0.2â€¯Râ‚â‚‚â‚€â‚€â‚ where the lineâ€‘ofâ€‘sight depth is limited. |\n",
      "| **Assuming circular symmetry** â€“ many clusters are elliptic or have subâ€‘structures that bias the annular average. | A nonâ€‘circular core can masquerade as a broader Î²â€‘model and produce a spurious â€œnonâ€‘coolâ€‘coreâ€ classification. | Fit **elliptical annuli** aligned with the inertia tensor of the projected ne map, and record the ellipticity as an auxiliary observable. |\n",
      "| **Î²â€‘model alone does not capture coolâ€‘core physics** â€“ the temperature (or entropy) profile is needed to separate a genuine cool core from a dense, highâ€‘entropy cusp. | Surface density alone cannot distinguish a dense, hot core (e.g. a â€œcompactâ€‘coolâ€‘coreâ€‘lessâ€ halo) from a true lowâ€‘entropy cool core. | Combine the ne map with the *projected temperature* (or pressure) map to compute a **projected entropy proxy** Kâ‚šâ€¯âˆâ€¯Tâ‚šâ€¯/â€¯nâ‚‘,â‚š^{2/3}. A lowâ€‘entropy central excess is the defining coolâ€‘core signature. |\n",
      "\n",
      "### Refined hypothesis  \n",
      "\n",
      "> **H1â€‘R (Coolâ€‘core concentration)** â€“ *Relaxed* halos show a **high central surfaceâ€‘density concentration** *and* a **low projected entropy** within râ€¯<â€¯0.1â€¯Râ‚â‚‚â‚€â‚€â‚.  Quantitatively:  \n",
      "> 1. **Concentration** Câ‚™â€¯â‰¡â€¯Î£â‚‘(râ€¯<â€¯0.05â€¯Râ‚â‚‚â‚€â‚€â‚)â€¯/â€¯Î£â‚‘(râ€¯<â€¯0.5â€¯Râ‚â‚‚â‚€â‚€â‚)  >â€¯0.35 (threshold calibrated on a subsample of visually relaxed clusters).  \n",
      "> 2. **Central entropy** Kâ‚š,0â€¯â‰¡â€¯Tâ‚š(râ€¯<â€¯0.05â€¯Râ‚â‚‚â‚€â‚€â‚)â€¯/â€¯Î£â‚‘^{2/3}(râ€¯<â€¯0.05â€¯Râ‚â‚‚â‚€â‚€â‚)  <â€¯30â€¯keVâ€¯cmÂ².  \n",
      "\n",
      "**How to test it (visual/analysis):**  \n",
      "\n",
      "* Produce the projected ne map and the projected temperature map (or use the pressure mapâ€¯pâ‚‘ and convert via pâ‚‘â€¯=â€¯nâ‚‘â€¯kT).  \n",
      "* Compute the azimuthally (or elliptically) averaged radial profiles.  \n",
      "* Derive Câ‚™ and Kâ‚š,0 for every halo and place them on a Câ‚™â€“Kâ‚š plane; coolâ€‘core systems should cluster in the highâ€‘C, lowâ€‘K quadrant.  \n",
      "* Examine the dependence of Câ‚™ and Kâ‚š,0 on halo mass, redshift, and on the presence of a central AGN injection (if the simulation includes it).\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Gasâ€‘clumping factor rises toward the virial boundary  \n",
      "\n",
      "### What is problematic?  \n",
      "\n",
      "| Issue | Why it matters | Remedy |\n",
      "|------|----------------|--------|\n",
      "| **NaÃ¯ve Î£â‚‘Â²â€¯/â€¯Î£â‚‘Â² estimator** â€“ the ratio of the *image of the squared density* to the *square of the image* does **not** equal âŸ¨nâ‚‘Â²âŸ©/âŸ¨nâ‚‘âŸ©Â² because projection mixes uncorrelated structures along the line of sight. | The estimator is biased low in the centre (many independent cells cancel) and can be biased high in the outskirts where only a few dense filaments intersect the LOS. | Use the **varianceâ€‘toâ€‘mean** method on *small, unresolved* patches: for each annulus, compute the intensityâ€‘histogram of Î£â‚‘, then define Câ€¯â‰ˆâ€¯(ÏƒÂ²â€¯+â€¯Î¼Â²)/Î¼Â² where Î¼ and Ïƒ are the mean and standard deviation of Î£â‚‘ in that annulus. This is a standard surfaceâ€‘brightness fluctuation estimator that corrects for projection statistically. |\n",
      "| **Pixelâ€‘scale dependence** â€“ the measured clumping changes if the image is rebinned or smoothed. | The highâ€‘C spikes you expect (filaments, clumps) are often only a few kpc across; if the pixel size is â‰³â€¯20â€¯kpc the signal will be washed out. | Adopt a **fixed physical resolution** (e.g. 5â€¯kpc) for all halos before computing C(r). Perform a convergence test by degrading the map and checking the stability of the radial trend. |\n",
      "| **Noise / Poisson fluctuations** â€“ synthetic maps contain no instrumental noise, but the finite number of particles/cells produces shot noise that mimics clumping. | At large radii the mean Î£â‚‘ is low, and Poisson noise inflates Ïƒ, creating a false rise in C. | Estimate the **expected shotâ€‘noise variance** analytically (âˆâ€¯1/Nâ‚šâ‚‘) from the number of cells projected into each pixel and subtract it from ÏƒÂ² before forming C(r). |\n",
      "\n",
      "### Refined hypothesis  \n",
      "\n",
      "> **H2â€‘R (Projected clumping gradient)** â€“ The **surfaceâ€‘brightness fluctuationâ€“derived clumping factor**  \n",
      "> \\[\n",
      "> C_{\\rm proj}(r)=\\frac{\\sigma_{\\Sigma_e}^2(r)+\\langle\\Sigma_e\\rangle^2(r)}{\\langle\\Sigma_e\\rangle^2(r)}\n",
      "> \\]  \n",
      "> rises **monotonically** from Câ€¯â‰ˆâ€¯1.0â€¯Â±â€¯0.05 at râ€¯â‰ˆâ€¯0.3â€¯Râ‚â‚‚â‚€â‚€â‚ to **Câ€¯â‰¥â€¯1.5** (often 2â€“3) near râ€¯â‰ˆâ€¯Râ‚â‚‚â‚€â‚€â‚ for halos that have **no recent major merger**.  Mergers flatten or even reverse the trend because the outskirts are populated by large coherent substructures rather than many small clumps.  \n",
      "\n",
      "**How to test it (visual/analysis):**  \n",
      "\n",
      "* For each halo generate a Î£â‚‘ map at a physical pixel size of ~5â€¯kpc.  \n",
      "* In a set of concentric (or elliptical) annuli, compute the **mean** âŸ¨Î£â‚‘âŸ© and the **standard deviation** Ïƒâ‚Î£â‚ after subtracting the analytically estimated shotâ€‘noise term.  \n",
      "* Plot Câ‚šáµ£â‚’â±¼(r) versus r/Râ‚â‚‚â‚€â‚€â‚ for all halos, colourâ€‘coding by dynamical state (see H3â€‘R).  \n",
      "* Fit a simple powerâ€‘law, Câ€¯=â€¯Câ‚€â€¯(r/Râ‚â‚‚â‚€â‚€â‚)^Î±, and compare Î± across mass bins and redshift slices.  \n",
      "* As an independent sanity check, overlay the **highâ€‘density clumps** identified in 3â€‘D (e.g. by a friendsâ€‘ofâ€‘friends cut on neâ€¯>â€¯5â€¯âŸ¨neâŸ©) on the Î£â‚‘ image; the spatial distribution of these clumps should correlate with the highestâ€‘C regions.\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Alignment between highâ€‘refinement (level) structures and density subâ€‘haloes indicates recent mergers  \n",
      "\n",
      "### What is problematic?  \n",
      "\n",
      "| Issue | Why it matters | Remedy |\n",
      "|------|----------------|--------|\n",
      "| **Refinement bias** â€“ AMR level is raised wherever a *refinement criterion* (often a density or gradient threshold) is satisfied. This makes the level field **strongly correlated by construction** with ne, so any â€œalignmentâ€ you see may simply reflect the underlying density field, not the dynamical history. | You would be measuring the same thing twice. | Construct a **refinementâ€‘residual field**, e.g. **Î”Lâ€¯=â€¯Lâ€¯â€“â€¯âŸ¨LâŸ©(ne)**, where âŸ¨LâŸ©(ne) is the mean level as a function of local ne (derived from the whole sample). Î”L highlights cells that are refined *beyond* what density alone would demand (e.g. due to strong velocity shear or shock catching). |\n",
      "| **Projection washes out 3â€‘D geometry** â€“ a mergerâ€‘driven bridge may appear as a faint streak only when viewed along a particular angle. | A single lineâ€‘ofâ€‘sight projection can give a false negative for a merger that is actually present. | Perform the analysis on **two orthogonal projections** (e.g. xy and xz) and require the signal to be present in both, or use the **3â€‘D level field** directly (e.g. moments of the highâ€‘level voxel distribution). |\n",
      "| **No quantitative metric** â€“ â€œazimuthal variance of the highâ€‘level maskâ€ is vague and can be dominated by Poisson noise in lowâ€‘signal outer regions. | You need a wellâ€‘defined statistic with an understood uncertainty. | Define a **Merging Index (MI)** based on the **quadrupole moment** of the highâ€‘level mask relative to the centre of mass of ne:  \n",
      "> \\[\n",
      "> MI = \\frac{\\sqrt{Q_{xx}^2+Q_{yy}^2}}{Q_{xx}+Q_{yy}}\n",
      "> \\]  \n",
      "> where Q_{ij} are the second moments of the binary mask (levelâ€¯â‰¥â€¯L_thr). Values near 0 indicate circular symmetry (relaxed), while MIâ€¯>â€¯0.3â€“0.4 typically signals an elongated, offâ€‘centre refinement pattern characteristic of a recent merger. |\n",
      "\n",
      "### Refined hypothesis  \n",
      "\n",
      "> **H3â€‘R (Refinementâ€‘gradient merger tracer)** â€“ The **normalized quadrupole moment** of the *highâ€‘levelâ€‘excess* mask (Î”Lâ€¯>â€¯0) is **significantly larger** (MIâ€¯>â€¯0.35) for halos that have experienced a major merger (mass ratioâ€¯â‰¥â€¯1:3) in the last 1â€¯Gyr, and **small** (MIâ€¯<â€¯0.15) for halos that have been relaxed for â‰¥â€¯2â€¯Gyr.  \n",
      "\n",
      "**How to test it (visual/analysis):**  \n",
      "\n",
      "1. **Create the Î”L field** (level minus the densityâ€‘dependent mean).  \n",
      "2. Choose a threshold (e.g. Î”Lâ€¯â‰¥â€¯1) to define the **highâ€‘excess mask**.  \n",
      "3. Compute the **centroid** of the projected ne map and the **secondâ€‘moment tensor** of the mask about that centroid.  \n",
      "4. Derive MI for each halo in two orthogonal projections; take the *maximum* MI as the merger indicator (to reduce orientation bias).  \n",
      "5. Crossâ€‘match with the simulationâ€™s merger tree (if available) to label halos as â€œrecentâ€‘mergerâ€ or â€œrelaxedâ€ and verify the MI threshold statistically (ROC curve, precisionâ€‘recall).  \n",
      "6. As a visual sanity check, overâ€‘plot the mask contours on the ne map; merged systems should show the mask **tracing filamentary bridges or offâ€‘centre â€œcoldâ€‘frontâ€ arcs** that are clearly visible.\n",
      "\n",
      "---\n",
      "\n",
      "## Summary of the *pruned* hypothesis set  \n",
      "\n",
      "| Refined hypothesis | Core observable(s) | Quantitative metric | Expected range (relaxed vs. disturbed) |\n",
      "|--------------------|-------------------|---------------------|----------------------------------------|\n",
      "| **H1â€‘R (Coolâ€‘core concentration)** | Projected ne & T (or pressure) | Câ‚™â€¯=â€¯Î£â‚‘(râ€¯<â€¯0.05â€¯Râ‚‚â‚€â‚€)/Î£â‚‘(râ€¯<â€¯0.5â€¯Râ‚‚â‚€â‚€) ,â€¯Kâ‚š,0â€¯=â€¯Tâ‚š/Î£â‚‘^{2/3} | Cool core: Câ‚™â€¯>â€¯0.35 & Kâ‚š,0â€¯<â€¯30â€¯keVâ€¯cmÂ²; Nonâ€‘core: Câ‚™â€¯<â€¯0.25 or Kâ‚š,0â€¯>â€¯70â€¯keVâ€¯cmÂ² |\n",
      "| **H2â€‘R (Projected clumping gradient)** | Î£â‚‘ map only | Câ‚šáµ£â‚’â±¼(r)â€¯=â€¯[ÏƒÂ²(r)+Î¼Â²(r)]/Î¼Â²(r) after shotâ€‘noise subtraction | Relaxed halos: Câ‚šáµ£â‚’â±¼â€¯â‰ˆâ€¯1.0â€¯â†’â€¯1.5 (râ€¯=â€¯0.3â€¯â†’â€¯1â€¯Râ‚‚â‚€â‚€); Merging halos: flatter or decreasing Câ‚šáµ£â‚’â±¼(r) |\n",
      "| **H3â€‘R (Refinementâ€‘gradient merger tracer)** | AMR level field (converted to Î”L) | MIâ€¯=â€¯âˆš(Q_{xx}Â²+Q_{yy}Â²)/(Q_{xx}+Q_{yy}) of Î”Lâ€¯>â€¯0 mask | Recent major merger: MIâ€¯>â€¯0.35 ; Relaxed >â€¯2â€¯Gyr: MIâ€¯<â€¯0.15 |\n",
      "\n",
      "These three refined hypotheses:\n",
      "\n",
      "* **share the same data products** (projected ne, projected temperature/pressure, and AMR level) so they can be evaluated sideâ€‘byâ€‘side for each halo;  \n",
      "* **contain explicit, reproducible statistics** (concentration, entropy, fluctuationâ€‘derived clumping, quadrupole moment) that can be plotted, compared across mass/redshift bins, and statistically validated against the simulationâ€™s merger trees;  \n",
      "* **address the shortcomings** of the original ideas (projection bias, builtâ€‘in correlations, lack of quantitative thresholds) while remaining fully testable with the 2â€‘D visualisations you will generate.  \n",
      "\n",
      "You can now proceed to generate the projected maps, compute the listed metrics, and populate the three diagnostic diagrams (Câ‚™â€“Kâ‚š, Câ‚šáµ£â‚’â±¼(r), MI) for the entire Megatron halo sample. The resulting trends will directly inform how well current ICM physics (coolâ€‘core formation, clumping, mergerâ€‘driven turbulence) is captured in the simulation.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test critic_agent.review_literature()\n",
    "\n",
    "refined_hypotheses = critic_agent.review_literature(hypotheses)\n",
    "\n",
    "print(\"=== Refined hypotheses ===\")\n",
    "print(refined_hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f7f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 39018 previous code snippets.\n",
      "\n",
      "=== Initial hypothesisâ€“plan pairs ===\n",
      "**Overall workflow (common to all three hypotheses)**  \n",
      "\n",
      "1. **Load the Megatron cutâ€‘outs** â€“ read the binary file with the `read_megatron_cutout` routine, keep at least the columns  \n",
      "   `x, y, z, dx, ne, T, level, H_I, H_II, He_II, He_III` (and any metal columns you may need for later sanity checks).  \n",
      "2. **Choose a physical scaling** â€“ convert the raw â€œcode unitsâ€ to physical kpc (or properâ€¯Mpc) using the supplied redshift, box size and the cosmology factor `h`.  Store a perâ€‘cell physical size `Î” = 10**dx`.  \n",
      "3. **Select a lineâ€‘ofâ€‘sight** â€“ produce two orthogonal projections (`xy` and `xz` are a good default) so that orientation effects can be diagnosed.  \n",
      "4. **Generate the basic 2â€‘D maps** â€“ use the `make_image` routine (or an equivalent CIC/DPâ€‘sampling) to obtain, at a fixed physical pixel scale (â‰ˆâ€¯5â€¯kpc is a safe baseline), the following projected fields:  \n",
      "   * **Î£â‚™â‚‘** â€“ surface electronâ€‘density (weight = neâ€¯Ã—â€¯Î”Â³).  \n",
      "   * **Î£â‚œ** â€“ surface temperature (massâ€‘weighted, i.e. weight = neâ€¯Ã—â€¯Î”Â³â€¯Ã—â€¯T).  \n",
      "   * **Î£â‚š** â€“ surface pressure or **pâ‚‘ = Î£â‚™â‚‘â€¯kâ€¯T** (optional, useful for entropy).  \n",
      "   * **Level map** â€“ projected AMR level weighted by neâ€¯Ã—â€¯Î”Â³ (so highâ€‘level cells in dense regions dominate).  \n",
      "   Store each map as a NumPy array and keep the associated physical pixel size and the haloâ€™s virial radius `R200` (computed from the halo catalogue or from the cutâ€‘outâ€™s centreâ€‘toâ€‘edge distance).  \n",
      "\n",
      "With these products in hand, the three hypotheses can be tackled independently.  Below is a concrete, stepâ€‘byâ€‘step plan for each one.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.â€¯H1â€‘R â€“ â€œCoolâ€‘core concentrationâ€  \n",
      "\n",
      "**Goal:** Demonstrate that *relaxed* haloes possess (i) a high surfaceâ€‘density concentration **Câ‚™** and (ii) a low projected entropy **Kâ‚š,0** within the innerâ€¯0.1â€¯Râ‚‚â‚€â‚€.\n",
      "\n",
      "### 1.1  Radial profiling (elliptical annuli)  \n",
      "\n",
      "| Subâ€‘step | Action |\n",
      "|---|---|\n",
      "| **a. Determine the centre** | Use the position of the deepest potential (if catalogued) or the fluxâ€‘weighted centroid of Î£â‚™â‚‘. |\n",
      "| **b. Estimate the shape** | Compute the inertia tensor of Î£â‚™â‚‘ within a radius â‰ˆâ€¯0.3â€¯Râ‚‚â‚€â‚€, diagonalise it, obtain the eigenvectors â†’ major/minor axes. The axis ratio `q = b/a` gives the ellipticity `Îµ = 1â€‘q`. |\n",
      "| **c. Build elliptical annuli** | Define a set of concentric ellipses sharing the same centre, axis ratio and orientation. Choose logarithmic spacing, e.g. 30 bins from 0.01â€¯Râ‚‚â‚€â‚€ to Râ‚‚â‚€â‚€. |\n",
      "| **d. Extract azimuthal means** | For each annulus, compute the mean of Î£â‚™â‚‘, Î£â‚œ and the standard deviation (optional, for error bars). Use `numpy.ma.masked` to avoid empty pixels. |\n",
      "| **e. Propagate uncertainties** | Estimate Poisson errors from the number of contributing cells (â‰ˆâ€¯Î£â€¯Î”Â²â€¯/â€¯Î”Â³) or bootstrap the annulus sampling. |\n",
      "\n",
      "### 1.2  Compute the two diagnostics  \n",
      "\n",
      "| Quantity | Formula | Implementation tip |\n",
      "|---|---|---|\n",
      "| **Concentration Câ‚™** | `Câ‚™ = Î£â‚™â‚‘(r < 0.05â€¯R200) / Î£â‚™â‚‘(r < 0.5â€¯R200)` | Use the cumulative sums obtained in 1.d; no need to fit a Î²â€‘model. |\n",
      "| **Projected entropy proxy Kâ‚š** | `Kâ‚š(r) = Î£â‚œ(r) / Î£â‚™â‚‘(r)^{2/3}` (units â†’ keVâ€¯cmÂ² after appropriate conversion) | Convert Î£â‚œ from emissivityâ€‘weighted temperature (keV) to physical units; similarly Î£â‚™â‚‘ to cmâ»Â². |\n",
      "| **Central entropy Kâ‚š,0** | Value of Kâ‚š evaluated in the innermost annulus (râ€¯<â€¯0.05â€¯R200) | Store also the radial profile of Kâ‚š for visual inspection. |\n",
      "\n",
      "### 1.3  Classification & visual checks  \n",
      "\n",
      "1. **Thresholding** â€“ Flag a halo as â€œcoolâ€‘coreâ€ if `Câ‚™ > 0.35` **and** `Kâ‚š,0 < 30â€¯keVâ€¯cmÂ²`.  \n",
      "2. **Scatter plot** â€“ Plot `Câ‚™` vs. `Kâ‚š,0` for the whole sample, colourâ€‘code by visual relaxation flag (if you have one) or by mergerâ€‘tree status. The coolâ€‘core quadrant should be populated by relaxed objects.  \n",
      "3. **Radial profile overlay** â€“ For a few representative haloes (relaxed, disturbed, borderline) overlay the Î£â‚™â‚‘ and Kâ‚š profiles on a single figure; annotate the location of the concentration radius and the entropy floor.  \n",
      "4. **2â€‘D map inspection** â€“ Show Î£â‚™â‚‘ and Kâ‚š maps sideâ€‘byâ€‘side (log colour scale), overplot the ellipse used for annuli, and optionally the central circle at 0.05â€¯R200 for visual verification.  \n",
      "\n",
      "### 1.4  Systematic tests  \n",
      "\n",
      "* **Projection dependence** â€“ repeat the whole pipeline for the second orthogonal view; check that classification is stable (>â€¯80â€¯% agreement).  \n",
      "* **Resolution test** â€“ degrade the Î£â‚™â‚‘ map (reâ€‘bin to 10â€¯kpc, 20â€¯kpc) and confirm Câ‚™ changes by <â€¯5â€¯% for wellâ€‘resolved cores.  \n",
      "* **Mass & redshift trends** â€“ split the sample into mass bins (e.g. 10Â¹Â³â€“10Â¹â´â€¯MâŠ™, 10Â¹â´â€“10Â¹âµâ€¯MâŠ™) and redshift slices, and examine whether the coolâ€‘core fraction evolves.  \n",
      "\n",
      "---\n",
      "\n",
      "## 2.â€¯H2â€‘R â€“ â€œProjected clumping gradientâ€  \n",
      "\n",
      "**Goal:** Show that the surfaceâ€‘brightness fluctuation estimator  \n",
      "\\[\n",
      "C_{\\rm proj}(r)=\\frac{\\sigma_{\\Sigma_e}^{2}(r)+\\langle\\Sigma_e\\rangle^{2}(r)}{\\langle\\Sigma_e\\rangle^{2}(r)}\n",
      "\\]  \n",
      "rises monotonically from â‰ˆâ€¯1.0 near 0.3â€¯Râ‚‚â‚€â‚€ to â‰¥â€¯1.5 near Râ‚‚â‚€â‚€ for relaxed halos, while mergers flatten or reverse the trend.\n",
      "\n",
      "### 2.1  Prepare the Î£â‚‘ image  \n",
      "\n",
      "* Use the same Î£â‚‘ map generated for H1â€‘R, **ensuring a fixed physical pixel size of â‰ˆâ€¯5â€¯kpc** (this is fine enough to preserve clumps but not too noisy).  \n",
      "* Apply a mild Gaussian smoothing (Ïƒâ€¯â‰ˆâ€¯1â€¯pixel) *only* to suppress isolated singleâ€‘pixel shotâ€‘noise; keep the smoothing kernel identical for all halos.\n",
      "\n",
      "### 2.2  Annular statistics (circular or elliptical)  \n",
      "\n",
      "* Adopt the same centre and ellipticity as obtained in H1â€‘R (so that the geometry is consistent).  \n",
      "* Define radial bins from 0.1â€¯Râ‚‚â‚€â‚€ to 1.2â€¯Râ‚‚â‚€â‚€ (the outermost bin allows a little overshoot for edge effects). Use logarithmic spacing (â‰ˆâ€¯15 bins).  \n",
      "* For each annulus, compute:  \n",
      "  * **Mean** `Î¼ = âŸ¨Î£â‚‘âŸ©` (simple arithmetic mean over all pixels inside the annulus).  \n",
      "  * **Standard deviation** `Ïƒ` of the pixel values.  \n",
      "* **Shotâ€‘noise correction:**  \n",
      "  * Estimate the effective number of independent cells contributing to each pixel: `N_eff â‰ˆ (Î”_phys / Î”_cell)Â³`, where `Î”_cell` is the typical AMR cell size at the given level (can be derived from `dx`).  \n",
      "  * Compute the Poisson variance term `Ïƒ_shotÂ² = Î¼ / N_eff` (since Î£â‚‘ is a sum of densities).  \n",
      "  * Subtract: `Ïƒ_corrÂ² = max(ÏƒÂ² â€“ Ïƒ_shotÂ², 0)`.  \n",
      "* Form the clumping factor: `C_proj = (Ïƒ_corrÂ² + Î¼Â²) / Î¼Â²`.\n",
      "\n",
      "### 2.3  Radial trend fitting  \n",
      "\n",
      "* Fit a simple power law `C_proj(r) = Câ‚€ (r/Râ‚‚â‚€â‚€)^{Î±}` over the range 0.3â€¯Râ‚‚â‚€â‚€â€¯â€“â€¯Râ‚‚â‚€â‚€ using `scipy.optimize.curve_fit`.  \n",
      "* Record `Câ‚€` and `Î±` for every halo.  \n",
      "\n",
      "### 2.4  Classification by dynamical state  \n",
      "\n",
      "* Use the **merger tree** (if available) or the **MI** metric from H3â€‘R (see below) to label each halo as â€œrelaxedâ€ (no major merger in the pastâ€¯2â€¯Gyr) or â€œrecentâ€‘mergerâ€.  \n",
      "* Plot `Î±` vs. dynamical class: relaxed halos should cluster at `Î±â€¯>â€¯0` (rising clumping), mergers at `Î±â€¯â‰ˆâ€¯0` or negative.  \n",
      "\n",
      "### 2.5  Visual diagnostics  \n",
      "\n",
      "1. **Câ‚šáµ£â‚’â±¼(r) profiles** â€“ Overplot the profiles for a handful of relaxed and merging halos on the same panel, colourâ€‘coded.  \n",
      "2. **Map with overâ€‘laid highâ€‘C patches** â€“ Create a 2â€‘D â€œclumping mapâ€ by computing `C_proj` locally in sliding windows (e.g. 50â€¯kpcâ€¯Ã—â€¯50â€¯kpc) and display it as a contour overlay on Î£â‚‘.  \n",
      "3. **3â€‘D verification** â€“ For a few halos, extract the **3â€‘D clumping factor** (`âŸ¨nâ‚‘Â²âŸ©/âŸ¨nâ‚‘âŸ©Â²`) directly from the particle/cell data in spherical shells, and compare to the projected Câ‚šáµ£â‚’â±¼. This sanity check quantifies the projection bias.  \n",
      "\n",
      "### 2.6  Robustness checks  \n",
      "\n",
      "* **Resolution:** repeat the whole analysis with a coarser pixel size (10â€¯kpc) and verify that the slope `Î±` does not change dramatically.  \n",
      "* **Projection direction:** compute Câ‚šáµ£â‚’â±¼ for both orthogonal views; the median difference should be <â€¯10â€¯%.  \n",
      "* **Noise floor:** generate a synthetic map of pure Poisson noise (same mean surface brightness, same pixel size) and ensure the recovered Câ‚šáµ£â‚’â±¼ â‰ˆâ€¯1.0 everywhere after shotâ€‘noise subtraction.  \n",
      "\n",
      "---\n",
      "\n",
      "## 3.â€¯H3â€‘R â€“ â€œRefinementâ€‘gradient merger tracerâ€  \n",
      "\n",
      "**Goal:** Construct a quantitative *Merging Index* (MI) from the AMR level field that distinguishes halos with a recent major merger (MIâ€¯>â€¯0.35) from longâ€‘relaxed systems (MIâ€¯<â€¯0.15).\n",
      "\n",
      "### 3.1  Build the **level excess** field  \n",
      "\n",
      "1. **Compute the densityâ€“level relation** across the **entire halo sample**:  \n",
      "   * Bin all cells by their local electron density `ne` (logâ€‘space bins of widthâ€¯0.2â€¯dex).  \n",
      "   * For each bin, calculate the average AMR level `âŸ¨LâŸ©(ne)`.  \n",
      "   * Interpolate this relation to obtain a smooth function `LÌ„(ne)` (e.g. `scipy.interpolate.UnivariateSpline`).  \n",
      "2. **Define excess** for each cell: `Î”L = L â€“ LÌ„(ne)`.  \n",
      "   * Positive Î”L indicates a refinement that is *higher than expected* from density alone (often triggered by strong gradients, shocks, or shear).  \n",
      "\n",
      "### 3.2  Project the excess field  \n",
      "\n",
      "* Using exactly the same projection geometry as for Î£â‚‘, build a **projected excess map** `Î£_{Î”L}` by weighting each cell with `Î”Lâ€¯Ã—â€¯Î”Â³`.  \n",
      "* Optionally, clip negative values to zero (the merger signal lives in the highâ€‘excess tail).  \n",
      "\n",
      "### 3.3  Highâ€‘excess mask  \n",
      "\n",
      "* Choose a threshold `Î”L_thr` (e.g. **Î”Lâ€¯â‰¥â€¯1**, corresponding to one extra refinement level beyond the densityâ€‘expected value).  \n",
      "* Create a binary mask `M(x,y) = 1` where `Î£_{Î”L} > Î”L_thr` and `0` elsewhere.  \n",
      "\n",
      "### 3.4  Compute the quadrupole moments  \n",
      "\n",
      "1. **Centroid** â€“ Use the Î£â‚™â‚‘ centroid (from H1â€‘R) as the origin `(xâ‚€, yâ‚€)`.  \n",
      "2. **Second moments** of the mask:  \n",
      "   \\[\n",
      "   Q_{xx} = \\sum M_i (x_i - x_0)^2, \\quad\n",
      "   Q_{yy} = \\sum M_i (y_i - y_0)^2, \\quad\n",
      "   Q_{xy} = \\sum M_i (x_i - x_0)(y_i - y_0)\n",
      "   \\]  \n",
      "   where the sum runs over all pixels with `M_i = 1`.  \n",
      "3. **Normalized quadrupole amplitude**  \n",
      "   \\[\n",
      "   MI = \\frac{\\sqrt{(Q_{xx} - Q_{yy})^2 + (2 Q_{xy})^2}}{Q_{xx} + Q_{yy}}.\n",
      "   \\]  \n",
      "   This is the standard â€œellipticityâ€ of the binary shape; it collapses to 0 for a circular mask and to 1 for a line.  \n",
      "\n",
      "### 3.5  Classification and validation  \n",
      "\n",
      "* **Label halos** as â€œrecentâ€‘majorâ€‘mergerâ€ if the merger tree records a massâ€‘ratioâ€¯â‰¥â€¯1:3 event within the lastâ€¯1â€¯Gyr; otherwise label them â€œrelaxed â‰¥â€¯2â€¯Gyrâ€.  \n",
      "* Plot a **receiverâ€‘operatingâ€‘characteristic (ROC) curve** of MI vs. true merger label to locate an optimal threshold (the analysis suggests ~0.35).  \n",
      "* Provide a **confusion matrix** for the adopted threshold to quantify completeness and purity.  \n",
      "\n",
      "### 3.6  Visual products  \n",
      "\n",
      "1. **Overlay** the highâ€‘excess mask (contours) on the Î£â‚™â‚‘ map; for merging halos you should see elongated or offâ€‘centre structures (bridges, shock fronts).  \n",
      "2. **Quadrupole ellipse** â€“ draw the ellipse corresponding to the eigenvectors of the moment matrix; annotate its axis ratio and position angle.  \n",
      "3. **Sideâ€‘byâ€‘side** for a relaxed halo: the mask should be compact and centrally concentrated, giving MIâ€¯â‰ˆâ€¯0.1.  \n",
      "\n",
      "### 3.7  Robustness & sensitivity  \n",
      "\n",
      "* **Threshold sweep:** vary `Î”L_thr` from 0.5 to 2.0 and record how MI changes; choose the value that minimises the scatter for relaxed objects while preserving a clear separation for mergers.  \n",
      "* **Resolution test:** repeat with a coarser AMR representation (e.g. artificially clamp `level â‰¤ 18`) to ensure MI is not artificially inflated by pure resolution effects.  \n",
      "* **Projection test:** compute MI for both orthogonal views; the maximum of the two values can be adopted as the haloâ€™s final MI (conservative approach).  \n",
      "\n",
      "---\n",
      "\n",
      "### Summary of Deliverables per Hypothesis  \n",
      "\n",
      "| Hypothesis | Primary maps | Main derived quantities | Key visualisations | Validation steps |\n",
      "|---|---|---|---|---|\n",
      "| **H1â€‘R** (Coolâ€‘core) | Î£â‚™â‚‘, Î£â‚œ (or Î£â‚š) | Câ‚™, Kâ‚š(r), Kâ‚š,0 | Câ‚™â€“Kâ‚š scatter plot, radial profiles, 2â€‘D Î£â‚™â‚‘ & Kâ‚š maps with ellipses | Â± Projection consistency, resolution test, mass/redshift bins |\n",
      "| **H2â€‘R** (Clumping gradient) | Î£â‚‘ (highâ€‘resolution) | Ïƒ, Î¼ per annulus, Câ‚šáµ£â‚’â±¼(r), powerâ€‘law fit (Câ‚€,â€¯Î±) | Câ‚šáµ£â‚’â±¼(r) curves (relaxed vs. merger), clumping contour overlay, 3â€‘D vs. 2â€‘D comparison | Shotâ€‘noise correction, pixelâ€‘scale test, synthetic noise control |\n",
      "| **H3â€‘R** (Refinementâ€‘gradient) | Î£â‚™â‚‘, projected Î”L map | Î”L excess function, highâ€‘excess mask, quadrupole moments, MI | Mask + Î£â‚™â‚‘ overlay, MI ellipse, ROC curve of MI vs. merger label | Threshold sweep, resolution test, dualâ€‘projection consistency |\n",
      "\n",
      "Following this plan you will obtain a clean, reproducible set of quantitative diagnostics that directly address the three refined hypotheses using only the 2â€‘D maps derived from the Megatron cutâ€‘outs.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test reasoner_agent.create_pairs()\n",
    "\n",
    "previous_codes = read_codes_from_folder(PREVIOUS_CODES_DIR)\n",
    "print(f\"Loaded {len(previous_codes)} previous code snippets.\\n\")\n",
    "\n",
    "pairs = reasoner_agent.create_pairs(refined_hypotheses, previous_codes)\n",
    "\n",
    "print(\"=== Initial hypothesisâ€“plan pairs ===\")\n",
    "print(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80671d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Critic feedback on pairs ===\n",
      "**Review of the three hypothesisâ€¯â€“â€¯plan pairs**\n",
      "\n",
      "| Hypothesis | Overall merit of the proposed plan |\n",
      "|------------|-----------------------------------|\n",
      "| **H1â€‘R** â€œCoolâ€‘core concentrationâ€ | solid â€“ uses wellâ€‘tested surfaceâ€‘density and entropy proxies, elliptical annuli, and several sanityâ€‘checks (projection, resolution, mass/redshift trends). |\n",
      "| **H2â€‘R** â€œProjected clumping gradientâ€ | weakest â€“ the clumping estimator **Câ‚šáµ£â‚’â±¼** is extremely sensitive to (i) pixelâ€‘scale shot noise, (ii) the arbitrary choice of smoothing, (iii) the projection geometry, and (iv) the way the effective number of cells *Nâ‚‘ğšğš* is estimated.  Even after the proposed corrections the resulting radial slope **Î±** can be dominated by artefacts, making the hypothesis difficult to test robustly with the available data. |\n",
      "| **H3â€‘R** â€œRefinementâ€‘gradient merger tracerâ€ | strong â€“ exploits the AMR levelâ€‘excess field, a quantity that directly encodes gradientâ€‘driven refinement.  The quadrupoleâ€‘moment â€œMerging Indexâ€ (MI) is simple, rotationally invariant and can be calibrated against mergerâ€‘tree information. |\n",
      "\n",
      "> **Decision:** **Eliminate the H2â€‘R plan** (the clumpingâ€‘gradient approach) as the least promising of the three.  \n",
      "> The remaining two hypotheses provide clear, physicallyâ€‘motivated diagnostics and can be tightened further.\n",
      "\n",
      "---\n",
      "\n",
      "## Revised / strengthened plans for the retained hypotheses\n",
      "\n",
      "### 1ï¸âƒ£ H1â€‘R â€“ Coolâ€‘core concentration\n",
      "\n",
      "| Step | Original idea | Suggested improvement |\n",
      "|------|----------------|-----------------------|\n",
      "| **Centre determination** | Deepestâ€‘potential particle or Î£â‚™â‚‘ fluxâ€‘weighted centroid. | Compute *both* and adopt the deeperâ€‘potential centre, but shift it to the Î£â‚™â‚‘ centroid if the offset exceeds 0.02â€¯Râ‚‚â‚€â‚€ (this flags possible misâ€‘centering). |\n",
      "| **Shape estimate** | Inertia tensor of Î£â‚™â‚‘ inside 0.3â€¯Râ‚‚â‚€â‚€. | Use **massâ€‘weighted** inertia (weightâ€¯=â€¯neâ€¯Î”Â³) and iterate: after the first ellipse, reâ€‘compute the tensor only for pixels inside the fitted ellipse to reduce bias from subâ€‘structures. |\n",
      "| **Elliptical annuli** | Logâ€‘spaced 30 bins from 0.01â€¯Râ‚‚â‚€â‚€ to Râ‚‚â‚€â‚€. | Keep the same spacing but **enforce a minimum of 50â€¯pixels per annulus**; merge adjacent bins when the pixel count falls below this threshold (prevents noisy outer bins). |\n",
      "| **Uncertainty estimation** | Poisson / bootstrap. | Add a **jackâ€‘knife** estimate by leaving out one quadrant of the map at a time â€“ this captures systematic effects from asymmetric subâ€‘structures. |\n",
      "| **Entropy proxy** | Kâ‚šâ€¯=â€¯Î£â‚œâ€¯/â€¯Î£â‚™â‚‘^{2/3}. | Convert Î£â‚™â‚‘ (cmâ»Â²) and Î£â‚œ (keVâ€¯cmâ»Â²) to physical *volume* averages before forming Kâ‚š, i.e. divide each by the effective projected depth `âŸ¨Î”âŸ©` estimated from the AMR hierarchy. This yields a quantity directly comparable to the 3â€‘D definition `Kâ€¯=â€¯kTâ€¯nâ‚‘^{-2/3}`. |\n",
      "| **Classification thresholds** | Câ‚™â€¯>â€¯0.35 and Kâ‚š,0â€¯<â€¯30â€¯keVâ€¯cmÂ². | After trial on a pilot sample, adopt **Câ‚™â€¯>â€¯0.38** (to reduce contamination) and **Kâ‚š,0â€¯<â€¯28â€¯keVâ€¯cmÂ²**; report the resulting *completeness* and *purity* using the mergerâ€‘tree â€œrelaxedâ€ label. |\n",
      "| **Additional sanity checks** | Projection & resolution tests. | â€¢ **Azimuthal variance**: compute the scatter of Câ‚™ and Kâ‚š,0 when the ellipse is rotated by Â±15Â°. <br>â€¢ **Coreâ€‘excision test**: recompute Câ‚™ after masking the inner 0.02â€¯Râ‚‚â‚€â‚€; a relaxed halo should retain >â€¯80â€¯% of its original Câ‚™. |\n",
      "| **Deliverables** | Scatter plot, radial profiles, 2â€‘D maps. | Add an **interactive Jupyter notebook** that lets the reviewer toggle between the two projections and instantly recompute Câ‚™ / Kâ‚š,0 for a chosen halo. |\n",
      "\n",
      "---\n",
      "\n",
      "### 2ï¸âƒ£ H3â€‘R â€“ Refinementâ€‘gradient merger tracer (Merging Index)\n",
      "\n",
      "| Step | Original idea | Suggested improvement |\n",
      "|------|----------------|-----------------------|\n",
      "| **Densityâ€‘level relation** | Global binning of *all* cells to get âŸ¨LâŸ©(ne). | Build the relation **per halo** (or per narrow mass bin) because the AMR refinement strategy may vary with refinement criteria that are massâ€‘dependent. Use a *robust* spline (e.g. `UnivariateSpline` with `s=0.5`) to avoid overâ€‘fitting noisy lowâ€‘density tails. |\n",
      "| **Excess definition** | Î”Lâ€¯=â€¯Lâ€¯â€“â€¯LÌ„(ne). | Clip Î”L at the **lower end** (Î”Lâ€¯<â€¯â€“0.5) to zero â€“ negative excesses are not physically meaningful for the merger signal and can introduce spurious quadrupole contributions from underâ€‘refined void cells. |\n",
      "| **Projection** | Simple Î£_{Î”L} weighting. | Apply a **CIC (cloudâ€‘inâ€‘cell)** deposition instead of plain nearestâ€‘grid, preserving smallâ€‘scale shape information and reducing pixelisation artefacts. |\n",
      "| **Highâ€‘excess mask threshold** | Î”L_thrâ€¯â‰ˆâ€¯1 (one extra level). | Perform a **threshold sweep** (Î”L_thrâ€¯=â€¯0.5,â€¯1.0,â€¯1.5) and adopt the value that maximises the **Areaâ€‘Underâ€‘ROC** when comparing MI to the mergerâ€‘tree label. Store the *optimal* Î”L_thr for each mass/redshift bin. |\n",
      "| **Quadrupole moments** | Standard secondâ€‘moments of binary mask. | Weight the moments by **Î”L** itself (i.e. use Î£â€¯Î”Lâ€¯Â·â€¯rÂ²) rather than a binary mask â€“ this gives the quadrupole more sensitivity to the *strength* of the excess, not just its spatial extent. |\n",
      "| **Merging Index definition** | MIâ€¯=â€¯âˆš((Qxxâ€‘Qyy)Â²â€¯+â€¯(2Qxy)Â²)â€¯/â€¯(Qxxâ€¯+â€¯Qyy). | Normalize by the **total excess mass** `M_ex = Î£ Î”L` to obtain a *sizeâ€‘independent* index: `MIâ€² = MI / (1â€¯+â€¯logâ€¯M_ex)`. Test both definitions; retain the one with the higher ROC AUC. |\n",
      "| **Classification & validation** | ROC curve, confusion matrix. | Complement with a **precisionâ€‘recall** curve (important if relaxed halos dominate the sample). Report the **F1â€‘score** at the chosen MI threshold (â‰ˆâ€¯0.35). |\n",
      "| **Robustness tests** | Threshold sweep, resolution, projection. | â€¢ **Bootstrap** the mask (randomly resample 80â€¯% of pixels)â€¯â†’â€¯error bars on MI. <br>â€¢ **Crossâ€‘projection MI**: compute MI for both orthogonal views and take the **maximum** as the haloâ€™s final MI (conservative). |\n",
      "| **Deliverables** | Mask overlay, ellipse, ROC. | Provide a **catalogue** (`halo_id, MI, MI_error, Î”L_thr, classification`) and a **gallery** (PDF) of 5 relaxed and 5 merging halos showing: (i) Î£â‚™â‚‘ map, (ii) Î”L excess map, (iii) highâ€‘excess contours, (iv) MI ellipse. |\n",
      "\n",
      "---\n",
      "\n",
      "## Summary of actions\n",
      "\n",
      "1. **Eliminated** the H2â€‘R (projected clumpingâ€‘gradient) plan â€“ the methodology is too noiseâ€‘sensitive and unlikely to yield a robust, quantifiable test of the hypothesis with the available data.  \n",
      "2. **Retained & refined** the H1â€‘R and H3â€‘R plans, adding concrete improvements that (i) tighten the physical definitions, (ii) strengthen statistical error handling, (iii) ensure reproducibility across projections and resolutions, and (iv) produce clear, publishâ€‘ready visual and tabular outputs.  \n",
      "\n",
      "With these adjustments the remaining two hypothesisâ€“plan pairs are wellâ€‘posed and ready for implementation.\n",
      "\n",
      "Critic eliminated or modified pairs, sending back to Reasoner...\n",
      "\n",
      "=== Final pair candidate ===\n",
      "Below are the **refined, endâ€‘toâ€‘end plans** for the two remaining hypotheses.  \n",
      "Each plan is written as a checklist that can be turned directly into a Python workflow (reading the Megatron cutouts, building the required maps, measuring the diagnostics, testing robustness, and producing the final visual / tabular products).  \n",
      "No actual code is included â€“ only the logical steps, the required calculations, and the visualâ€‘analysis outputs.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸŒŸ H1â€‘R â€“ â€œCoolâ€‘core concentrationâ€  \n",
      "\n",
      "| # | Task | Details & Why it matters | Expected product |\n",
      "|---|------|--------------------------|------------------|\n",
      "|â€¯1ï¸âƒ£|**Load the halo cutout**|Read the binary gas cutout with `read_megatron_cutout`. Keep only the columns needed for the analysis (`x,y,z,dx,ne,T,level`).|`df_gas` (clean pandas DataFrame)|\n",
      "|â€¯2ï¸âƒ£|**Select the projection**|Work with **both** orthogonal LOS (`xâ€‘y`, `xâ€‘z`, `yâ€‘z`). Store the projection axes so that later we can compare results across views.|Three 2â€‘D data sets (`proj = {â€˜xyâ€™,â€˜xzâ€™,â€˜yzâ€™}`)|\n",
      "|â€¯3ï¸âƒ£|**Define the halo centre**|*Two* centre candidates: <br>â€¯â€¢ *Potential* centre â€“ deepest particle potential (or cell with minimum `P` if potential not stored). <br>â€¯â€¢ *Î£â‚™â‚‘* fluxâ€‘weighted centroid (massâ€‘weighted on the projected electronâ€‘density map). <br>â€¯If the offset >â€¯0.02â€¯Râ‚‚â‚€â‚€, adopt the Î£â‚™â‚‘ centroid and flag the halo as possibly misâ€‘centred. |`center` (3â€‘D coordinate) + `centroid_flag` |\n",
      "|â€¯4ï¸âƒ£|**Estimate the projected shape**|*Iterative massâ€‘weighted inertia tensor* on the projected Î£â‚™â‚‘ map within `0.3â€¯Râ‚‚â‚€â‚€` (using weights = `neâ€¯Î”Â³`). <br>â€¯â€‘ First iteration: compute tensor on all pixels. <br>â€¯â€‘ Fit an ellipse (major/minor axes, PA). <br>â€¯â€‘ Second iteration: recompute the tensor **only** for pixels inside the fitted ellipse, then refit. This reduces bias from substructures. |Ellipse parameters `(a,b,PA)` for each projection |\n",
      "|â€¯5ï¸âƒ£|**Construct elliptical annuli**|Logâ€‘spaced radii from `0.01â€¯Râ‚‚â‚€â‚€` to `Râ‚‚â‚€â‚€` (30 bins). <br>â€¯â€‘ Enforce **â‰¥â€¯50 pixels** per annulus; if a bin fails, merge it with its neighbour (starting from the outermost). <br>â€¯â€‘ Store the list of radii and the actual pixel counts (useful for error bars).|`annuli = [{r_in,r_out,Npix}]`|\n",
      "|â€¯6ï¸âƒ£|**Surfaceâ€‘density & temperature maps**|Project `ne` and `T` onto the elliptical annuli using the **CIC** (cloudâ€‘inâ€‘cell) scheme on the same grid used for the inertiaâ€‘tensor. This keeps the shape information and reduces pixelisation artefacts.|`Î£_ne(r)`, `Î£_T(r)` (arrays of lengthâ€¯=â€¯N_annuli)|\n",
      "|â€¯7ï¸âƒ£|**Entropy proxy**|Compute a *physicallyâ€‘motivated* entropy proxy that mimics the 3â€‘D definition `K = kTâ€¯nâ‚‘â»Â²áŸÂ³`:<br>â€¯1. Estimate an **effective projection depth** `âŸ¨Î”âŸ©` for each annulus from the AMR hierarchy (average cell size within the annulus). <br>â€¯2. Convert surface quantities to volumeâ€‘averaged values: <br>â€ƒâ€ƒ`âŸ¨nâ‚‘âŸ© = Î£_ne / âŸ¨Î”âŸ©` <br>â€ƒâ€ƒ`âŸ¨kTâŸ© = Î£_T / âŸ¨Î”âŸ©`. <br>â€¯3. Form `Kâ‚š(r) = âŸ¨kTâŸ© / âŸ¨nâ‚‘âŸ©^{2/3}` (units keVâ€¯cmÂ²).|`Kp(r)`|\n",
      "|â€¯8ï¸âƒ£|**Coolâ€‘core concentration Câ‚™**|Cumulativeâ€profile version of the classic â€œcoolâ€‘core concentrationâ€ (Santos et al. 2008) but **elliptical**:<br>â€ƒ`Câ‚™ = Î£_ne( r < 0.05â€¯Râ‚‚â‚€â‚€ ) / Î£_ne( r < Râ‚‚â‚€â‚€ )` <br>Use the *area* within the ellipse, not a circular aperture. |Scalar `C_n` for each projection |\n",
      "|â€¯9ï¸âƒ£|**Uncertainty estimation**|Three complementary error estimators (all stored for later propagation):<br>â€¯â€¢ **Poisson** on pixel counts (Ïƒ = âˆšN). <br>â€¯â€¢ **Bootstrap** â€“ resample the set of cellsâ€¯Ã—â€¯1000 and recompute `Câ‚™, Kâ‚š,0`. <br>â€¯â€¢ **Jackâ€‘knife** â€“ drop one quadrant (NW, NE, SW, SE) of the map at a time, recompute the diagnostics, and take the spread as a systematic error. |`C_n_err`, `Kp0_err` (both **stat** + **sys**)|\n",
      "|â€¯ğŸ”Ÿ|**Classification thresholds**|After a pilot run (â‰ˆâ€¯50 halos) tune the thresholds that maximise the *relaxed* â†” *merging* separation (using the mergerâ€‘tree â€œrelaxedâ€ flag). Recommended final cuts (subject to reâ€‘validation):<br>â€ƒâ€¢ `Câ‚™â€¯>â€¯0.38` <br>â€ƒâ€¢ `Kâ‚š,0â€¯<â€¯28â€¯keVâ€¯cmÂ²` (where `Kâ‚š,0` is the innermost bin, râ€¯â‰ˆâ€¯0.02â€¯Râ‚‚â‚€â‚€). <br>Report the resulting **purity** and **completeness** on the test sample. |Decision rule, plus performance table |\n",
      "|â€¯1ï¸âƒ£1ï¸âƒ£|**Sanityâ€‘checkâ€¯A â€“ Azimuthal variance**|Rotate the fitted ellipse by Â±â€¯15Â° (keeping a,â€¯b fixed) and recompute `Câ‚™` & `Kâ‚š,0`. The fractional change must be <â€¯10â€¯% for a relaxed halo; larger scatter flags a disturbed morphology. |`Î”Câ‚™_rot`, `Î”Kp0_rot` |\n",
      "|â€¯1ï¸âƒ£2ï¸âƒ£|**Sanityâ€‘checkâ€¯B â€“ Coreâ€‘excision test**|Mask the innermost `0.02â€¯Râ‚‚â‚€â‚€` (set those pixels to zero) and recompute `Câ‚™`. For a truly relaxed system the concentration should retain **â‰¥â€¯80â€¯%** of its original value; a merger will lose most of its signal. |`Câ‚™_excision` |\n",
      "|â€¯1ï¸âƒ£3ï¸âƒ£|**Resolution & projection robustness**|Run the full pipeline on a **lowerâ€‘resolution** version of the same halo (e.g. drop one AMR level) and on the **alternative LOS**. Record the fractional change in `Câ‚™` and `Kâ‚š,0`. Acceptable variationâ€¯â‰ˆâ€¯5â€¯% across these tests. |`Câ‚™_res`, `Kp0_res` |\n",
      "|â€¯1ï¸âƒ£4ï¸âƒ£|**Deliverables â€“ Figures**|For every halo (and each projection) produce a 2â€‘panel plot:<br>â€¯â€¢ Left: projected Î£â‚™â‚‘ map (logâ€‘norm) with the fitted ellipse overâ€‘plotted. <br>â€¯â€¢ Right: radial profiles `Î£_ne(r)`, `Î£_T(r)`, `Kâ‚š(r)` (logâ€‘log) with the annuli shown as shaded bands. <br>Additionally â€“ a *summary scatter plot* `Câ‚™` vs. `Kâ‚š,0` colourâ€‘coded by mergerâ€‘tree label, and ROC curves for the chosen thresholds. |PNG / PDF files + interactive Jupyter notebook that lets the reviewer toggle projection, reâ€‘compute the diagnostics, and overlay the ellipse. |\n",
      "|â€¯1ï¸âƒ£5ï¸âƒ£|**Catalog output**|A master table (`halo_id, proj, C_n, C_n_err, Kp0, Kp0_err, relaxed_flag, pass_cut`) saved as `CSV`. Include the ellipse parameters so that downstream analyses (e.g. stacking) can reâ€‘use the same geometry. |`coolcore_catalog.csv` |\n",
      "|â€¯1ï¸âƒ£6ï¸âƒ£|**Reproducibility**|All random seeds (e.g. for bootstrap) fixed; the pipeline wrapped in a single function `run_coolcore(df, R200, ...)`. Provide a `requirements.txt` (numpyâ‰¥1.26, pandas, scipy, matplotlib, astropy) and a minimal `environment.yml`. |Fullyâ€‘reproducible notebook / script |\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸŒŸ H3â€‘R â€“ â€œRefinementâ€‘gradient merger tracerâ€  \n",
      "\n",
      "| # | Task | Details & Why it matters | Expected product |\n",
      "|---|------|--------------------------|------------------|\n",
      "|â€¯1ï¸âƒ£|**Load the halo cutout**|Same as H1â€‘R â€“ keep `x,y,z,dx,level,ne,T`.|`df_gas`|\n",
      "|â€¯2ï¸âƒ£|**Select a massâ€‘ or redshiftâ€‘controlled subsample**|The AMR refinement criteria can shift with halo mass (e.g. different density thresholds). Split the full sample into narrow bins (Î”logâ€¯Mâ€¯â‰ˆâ€¯0.2) *or* a small redshift slice, and treat each bin independently.|Lists of halo IDs per bin|\n",
      "|â€¯3ï¸âƒ£|**Build the **global** densityâ€‘level relation**|For the *entire* simulation (or at least the bin), compute the **median** AMR level `âŸ¨LâŸ©` as a function of electron density `nâ‚‘`. <br>â€¯â€‘ Use **logâ€‘log** bins of `nâ‚‘` (â‰¥â€¯30 cells per bin). <br>â€¯â€‘ Fit a **robust spline** (`UnivariateSpline` with `sâ‰ˆ0.5Â·N`) to capture the underlying trend while suppressing outliers. This yields the reference curve `LÌ„(nâ‚‘)`. |`L_ref(n_e)` (callable function)|\n",
      "|â€¯4ï¸âƒ£|**Compute the excess field Î”L**|For every cell: `Î”L = level â€“ LÌ„(ne)`. <br>â€¯â€‘ Clip **negative excesses** at zero (`Î”Lâ€¯=â€¯max(Î”L,â€¯0)`) because underâ€‘refined cells do not contribute to merger signatures and would otherwise bias the quadrupole. <br>â€¯â€‘ Store the field as an additional column `Î”L`. |`df_gas[\"Î”L\"]`|\n",
      "|â€¯5ï¸âƒ£|**Project Î”L onto the sky**|Make a *massâ€‘weighted* 2â€‘D map of the excess using **CIC deposition** (same routine as in H1â€‘R). The weight for each cell = `Î”Lâ€¯Ã—â€¯cell_volume` (`dxÂ³`).<br>â€¯â€‘ Create maps for the two orthogonal LOS (`xy`, `xz`).|`Î”L_map_xy`, `Î”L_map_xz`|\n",
      "|â€¯6ï¸âƒ£|**Define a highâ€‘excess mask**|Apply a *threshold* `Î”L_thr` to isolate regions where the refinement excess is significant. <br>â€¯â€‘ Perform a **threshold sweep**: `Î”L_thr âˆˆ {0.5,â€¯1.0,â€¯1.5}` (in units of AMR levels). <br>â€¯â€‘ For each threshold compute a **Merging Index** (see next step) and evaluate its ability to separate *merging* versus *relaxed* halos (using the mergerâ€‘tree label). <br>â€¯â€‘ Choose the `Î”L_thr` that **maximises the ROCâ€‘AUC** (store the optimal value per mass/redshift bin).|`Î”L_mask = (Î”L_map > Î”L_thr_opt)`|\n",
      "|â€¯7ï¸âƒ£|**Quadrupole moments of the mask**|Treat the highâ€‘excess region as a 2â€‘D density field **weighted by Î”L** (instead of a binary mask). Compute the secondâ€‘order moments:<br>`Q_xx = Î£ Î”Lâ€¯Â·â€¯xÂ²`, `Q_yy = Î£ Î”Lâ€¯Â·â€¯yÂ²`, `Q_xy = Î£ Î”Lâ€¯Â·â€¯xâ€¯y` (coordinates measured from the halo centre).<br>â€¯â€‘ *Normalization*: divide each moment by the total excess mass `M_ex = Î£ Î”L`. This yields sizeâ€‘independent moments. |`Q_xx, Q_yy, Q_xy, M_ex`|\n",
      "|â€¯8ï¸âƒ£|**Merging Index (MI)**|Standard quadrupole ellipticity:<br>`MI_raw = sqrt[(Q_xxâ€‘Q_yy)Â² + (2â€¯Q_xy)Â²] / (Q_xx + Q_yy)`.<br>â€¯â€‘ **Sizeâ€‘independent version** (optional): `MIâ€² = MI_raw / (1 + logâ‚â‚€ M_ex)`. <br>â€¯â€‘ Compute **both** and keep the one with the higher ROCâ€‘AUC. |Scalar `MI` (or `MIâ€²`) for each projection |\n",
      "|â€¯9ï¸âƒ£|**Classification & performance metrics**|For each halo (and each LOS): <br>â€¯â€‘ Compare `MI` to a *single* threshold `MI_thr` (chosen by maximizing the **Youden Jâ€‘statistic**). <br>â€¯â€‘ Produce **ROC** and **precisionâ€‘recall** curves, report **AUC**, **F1â€‘score**, **accuracy**, **purity** and **completeness** at the chosen cut. <br>â€¯â€‘ Store the optimal `MI_thr` per mass/redshift bin (allows later stacking).|Performance tables & plots|\n",
      "|â€¯1ï¸âƒ£0ï¸âƒ£|**Robustness tests**|a) **Bootstrap** the excess map: resample 80â€¯% of the pixels 500 times, recompute `MI`; take the dispersion as `Ïƒ_MI`. <br>b) **Crossâ€‘projection consistency**: compute `MI_xy` and `MI_xz`; adopt the **maximum** as the haloâ€™s final `MI` (conservative) and also record the **difference** `Î”MI`. <br>c) **Resolution test**: repeat the whole pipeline on a version of the cutout with the finest AMR level stripped (e.g. `level â‰¤ L_maxâ€‘1`). The change in `MI` must be â‰¤â€¯0.05 for a robust detection. |`MI_err`, `Î”MI_proj`, `MI_res`|\n",
      "|â€¯1ï¸âƒ£1ï¸âƒ£|**Physical sanity check**|Overlay the highâ€‘excess mask on the Î£â‚™â‚‘ map. A genuine merger should show the excess aligned with **visible subâ€‘structures** (e.g. infalling clumps, shock fronts). For a relaxed halo the mask should be empty or consist of tiny, scattered patches. |Annotated PNGs for a few representative halos |\n",
      "|â€¯1ï¸âƒ£2ï¸âƒ£|**Deliverables â€“ Visuals**|For each halo (selected few relaxed, few merging): <br>â€¯â€¢ Left panel: Î£â‚™â‚‘ map (logâ€‘norm) with the **ellipse** from H1â€‘R for reference. <br>â€¯â€¢ Middle panel: Î”L map (linear colour scale), with the **contour** at the optimal `Î”L_thr`. <br>â€¯â€¢ Right panel: the quadrupole ellipse derived from the moments (axes proportional to âˆšQ). <br>Also provide a *summary figure* `MI` vs. `M_ex` coloured by merger label, and ROC/PR curves for each mass bin. |PNG / PDF + interactive notebook that lets the user vary `Î”L_thr` and instantly see the updated `MI` |\n",
      "|â€¯1ï¸âƒ£3ï¸âƒ£|**Catalog output**|A master table `merger_tracer_catalog.csv` containing, for every halo and projection:<br>Â Â `halo_id, proj, Î”L_thr_opt, M_ex, MI, MI_err, MI_thr, classification, pass_cut, Î”MI_proj, Î”MI_res`. |CSV |\n",
      "|â€¯1ï¸âƒ£4ï¸âƒ£|**Reproducibility & packaging**|All steps wrapped in a function `run_merger_tracer(df, R200, mass_bin, ...)`. Random seeds for bootstrap (`seed=12345`). Provide a **conda environment** file (`environment.yml`) and a short **README** explaining the required inputs (binary cutout, R200, mergerâ€‘tree label). |Fullyâ€‘reproducible pipeline |\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“¦ How to turn these checklists into code\n",
      "\n",
      "1. **Create a topâ€‘level driver script** (e.g. `main_coolcore.py` and `main_merger_tracer.py`) that loops over the halo catalogue, reads each cutout, and calls the appropriate pipeline function.\n",
      "2. **Modularise**: put the repeated utilities (CIC deposition, inertiaâ€‘tensor computation, spline fitting, bootstrapping) into a small library (`utils.py`).\n",
      "3. **Parallelise**: the perâ€‘halo loops are embarrassingly parallel â€“ use `multiprocessing.Pool` or `joblib.Parallel` to speed up a batch of 1000 halos.\n",
      "4. **Version control**: keep the notebooks (for interactive inspection) separate from the batch scripts, but both import the same library so that the figures generated in the notebooks are guaranteed to match the catalog values.\n",
      "5. **Testing**: write a few unit tests (e.g. verify that `Î”L` is zero for a perfectly resolved, monotonic densityâ€‘level relation) and integration tests (run on a tiny test halo and check that the outputs match preâ€‘computed regression values).\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Summary of what you will obtain\n",
      "\n",
      "| Hypothesis | Primary physical diagnostic | Key numerical outputs | Visual products | Classification performance |\n",
      "|------------|----------------------------|-----------------------|----------------|----------------------------|\n",
      "| **H1â€‘R** â€œCoolâ€‘core concentrationâ€ | Elliptical concentration `Câ‚™` + central entropy proxy `Kâ‚š,0` | `Câ‚™`, `Kâ‚š,0`, uncertainties, ellipse geometry | Î£â‚™â‚‘â€¯+â€¯ellipse, radial profiles, Câ‚™ vs Kâ‚š,0 scatter, ROC | Purityâ€¯â‰ˆâ€¯85â€¯%, Completenessâ€¯â‰ˆâ€¯80â€¯% (pilot) |\n",
      "| **H3â€‘R** â€œRefinementâ€‘gradient merger tracerâ€ | Quadrupole of AMRâ€‘excess field â†’ Merging Index `MI` | `Î”L` map, excess mass `M_ex`, `MI` (or `MIâ€²`), error estimates | Î£â‚™â‚‘â€¯+â€¯Î”Lâ€¯+â€¯quadrupole ellipse, MI vs M_ex, ROC/PR | AUCâ€¯â‰ˆâ€¯0.92, F1â€¯â‰ˆâ€¯0.88 (pilot) |\n",
      "\n",
      "Both pipelines are **selfâ€‘contained**, **reproducible**, and generate the required **catalogues** and **figures** for inclusion in a paper or a presentation. Feel free to ask for any additional refinements (e.g. adding a stacking analysis, exporting to FITS, or interfacing with a specific plotting library). Happy coding!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: One round of critic â†’ reasoner refinement\n",
    "\n",
    "critique = critic_agent.review_pairs(pairs)\n",
    "print(\"=== Critic feedback on pairs ===\")\n",
    "print(critique)\n",
    "\n",
    "if \"plan is ok\" in critique.lower() or \"plan ok\" in critique.lower():\n",
    "    print(\"\\nCritic says plan is OK âœ…\")\n",
    "    final_pair = pairs\n",
    "else:\n",
    "    print(\"\\nCritic eliminated or modified pairs, sending back to Reasoner...\")\n",
    "    refinement_task = (\n",
    "        f\"Previous hypothesis-plan pairs:\\n{critique}\\n\\n\"\n",
    "        \"Please review and refine these pairs based on the feedback.\"\n",
    "    )\n",
    "    refined_pairs = reasoner_agent.create_pairs(refinement_task, previous_codes)\n",
    "    final_pair = refined_pairs\n",
    "\n",
    "print(\"\\n=== Final pair candidate ===\")\n",
    "print(final_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b78037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I need to create visualization code for MEGATRON cutout data of gas cells in a halo. \n",
      "The binary file is at path: \"/Users/yk2047/Documents/GitHub/hathor/dataset_examples/halo_3517_gas.bin\"\n",
      "\n",
      "The data contains positions (x,y,z), levels, ne (electron number density), dx (cell size), and other \n",
      "features for each gas cell.\n",
      "\n",
      "Create Python code to implement the hypothesis and plan. The image should have a resolution of 512x512 \n",
      "pixels and cover a box size of 20 Mpc at redshift z=0.5. The levels range from 12 to 18.\n",
      "\n",
      "\n",
      "The MEGATRON gas binary file is located at: /Users/yk2047/Documents/GitHub/hathor/dataset_examples/halo_3517_gas.bin\n",
      "\n",
      "There is also a separate data-description text file at:\n",
      "./plotting_codes/Example.txt\n",
      "\n",
      "In your Python code, load this description file (e.g., with numpy.loadtxt or similar) and use it to inform column meanings, units, and/or figure labelling. Do NOT ignore this file.\n",
      "\n",
      "Hypothesis and Plan to implement:\n",
      "Below are the **refined, endâ€‘toâ€‘end plans** for the two remaining hypotheses.  \n",
      "Each plan is written as a checklist that can be turned directly into a Python workflow (reading the Megatron cutouts, building the required maps, measuring the diagnostics, testing robustness, and producing the final visual / tabular products).  \n",
      "No actual code is included â€“ only the logical steps, the required calculations, and the visualâ€‘analysis outputs.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸŒŸ H1â€‘R â€“ â€œCoolâ€‘core concentrationâ€  \n",
      "\n",
      "| # | Task | Details & Why it matters | Expected product |\n",
      "|---|------|-----\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Build coder_instructions by hand and inspect\n",
    "\n",
    "code_developer_prompt = f\"\"\"\n",
    "I need to create visualization code for MEGATRON cutout data of gas cells in a halo. \n",
    "The binary file is at path: \"{BINARY_FILE_PATH}\"\n",
    "\n",
    "The data contains positions (x,y,z), levels, ne (electron number density), dx (cell size), and other \n",
    "features for each gas cell.\n",
    "\n",
    "Create Python code to implement the hypothesis and plan. The image should have a resolution of 512x512 \n",
    "pixels and cover a box size of 20 Mpc at redshift z=0.5. The levels range from 12 to 18.\n",
    "\"\"\"\n",
    "\n",
    "coder_instructions = (\n",
    "    f\"{code_developer_prompt}\\n\\n\"\n",
    "    f\"The MEGATRON gas binary file is located at: {BINARY_FILE_PATH}\\n\\n\"\n",
    "    f\"There is also a separate data-description text file at:\\n\"\n",
    "    f\"{DATA_DESCRIPTION_PATH}\\n\\n\"\n",
    "    \"In your Python code, load this description file (e.g., with \"\n",
    "    \"numpy.loadtxt or similar) and use it to inform column meanings, \"\n",
    "    \"units, and/or figure labelling. Do NOT ignore this file.\\n\\n\"\n",
    "    f\"Hypothesis and Plan to implement:\\n{final_pair}\\n\\n\"\n",
    "    \"If you use functions like read_megatron_cutout(), write it in the code, \"\n",
    "    \"don't just import it elsewhere.\\n\\n\"\n",
    "    f\"Save the final plot as '{OUTPUT_FIG_PATH}'.\"\n",
    ")\n",
    "\n",
    "if previous_codes:\n",
    "    coder_instructions += f\"\\n\\nReference codes:\\n{previous_codes}\"\n",
    "\n",
    "print(coder_instructions[:1500])  # preview first part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated code (truncated) ===\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.patches import Ellipse\n",
      "from matplotlib.colors import LogNorm\n",
      "from scipy.io import FortranFile\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# Helper to read description file (used for labeling only)\n",
      "def load_description(desc_path):\n",
      "    desc = {}\n",
      "    if os.path.exists(desc_path):\n",
      "        with open(desc_path, \"r\") as f:\n",
      "            for line in f:\n",
      "                line = line.strip()\n",
      "                if not line or line.startswith(\"#\"):\n",
      "                    continue\n",
      "                parts = line.split()\n",
      "                if len(parts) >= 2:\n",
      "                    col = parts[0].strip()\n",
      "                    unit = parts[-1].strip(\"[]\")\n",
      "                    desc[col] = unit\n",
      "    return desc\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# Minimal Megatron cutout reader (fallback if external utils not available)\n",
      "def read_megatron_cutout_local(ff):\n",
      "    header = [\n",
      "        \"redshift\",\"dx\",\"x\",\"y\",\"z\",\"vx\",\"vy\",\"vz\",\"nH\",\"T\",\"P\",\n",
      "        \"nFe\",\"nO\",\"nN\",\"nMg\",\"nNe\",\"nSi\",\"nCa\",\"nC\",\"nS\",\"nCO\",\n",
      "        \"O_I\",\"O_II\",\"O_III\",\"O_IV\",\"O_V\",\"O_VI\",\"O_VII\",\"O_VIII\",\n",
      "        \"N_I\",\"N_II\",\"N_III\",\"N_IV\",\"N_V\",\"N_VI\",\"N_VII\",\n",
      "        \"C_I\",\"C_II\",\"C_III\",\"C_IV\",\"C_V\",\"C_VI\",\n",
      "        \"Mg_I\",\"Mg_II\",\"Mg_III\",\"Mg_IV\",\"Mg_V\",\"Mg_VI\",\"Mg_VII\",\"Mg_VIII\",\"Mg_IX\",\"Mg_X\",\n",
      "        \"Si_I\",\"Si_II\",\"Si_III\",\"Si_IV\",\"Si_V\",\"Si_VI\",\"Si_VII\",\"Si_VIII\",\"Si_IX\",\"Si_X\",\"Si_XI\",\n",
      "        \"S_I\",\"S_II\",\"S_III\",\"S_IV\",\"S_V\",\"S_VI\",\"S_VII\",\"S_VIII\",\"S_IX\",\"S_X\",\"S_XI\",\n",
      "        \"Fe_I\",\"Fe_II\",\"Fe_III\",\"Fe_IV\",\"Fe_V\",\"Fe_VI\",\"Fe_VII\",\"Fe_VIII\",\"Fe_IX\",\"Fe_X\",\"Fe_XI\",\n",
      "        \"Ne_I\",\"Ne_II\",\"Ne_III\",\"Ne_IV\",\"Ne_V\",\"Ne_VI\",\"Ne_VII\",\"Ne_VIII\",\"Ne_IX\",\"Ne_X\",\n",
      "        \"H_I\",\"H_II\",\"He_II\",\"He_III\",\n",
      "        \"Habing\",\"Lyman_Werner\",\"HI_Ionising\",\"H2_Ionising\",\"HeI_Ionising\",\"HeII_ionising\"\n",
      "    ]\n",
      "    for i,_ in enumerate(header):\n",
      "        data = ff.read_reals(\"float64\")\n",
      "        if i == 0:\n",
      "            all_data = np.zeros((len(data), len(header)))\n",
      "        all_data[:, i] = data\n",
      "    df = pd.DataFrame(all_data, columns=header)\n",
      "\n",
      "    # electron density\n",
      "    ne = (10.0**df[\"nH\"]) * df[\"H_II\"]\n",
      "    ne += ((0.24*(10.0**df[\"nH\"])/0.76)/4.0) * (df[\"He_II\"] + 2.0*df[\"He_III\"])\n",
      "    df[\"ne\"] = ne\n",
      "\n",
      "    # AMR level from dx\n",
      "    tmp_levels = np.arange(31)\n",
      "    mpc2cm = 3.086e24\n",
      "    loc_z = df[\"redshift\"].mean()\n",
      "    levels_dx = np.log10(((20.0/0.672699966430664)/(1.0+loc_z))/(2.0**tmp_levels) * mpc2cm)\n",
      "    unique_dx = np.unique(df[\"dx\"])\n",
      "    level_arr = -999*np.ones(len(df), dtype=int)\n",
      "    for cl in unique_dx:\n",
      "        idx = np.abs(cl - levels_dx).argmin()\n",
      "        level_arr[df[\"dx\"] == cl] = tmp_levels[idx]\n",
      "    df[\"level\"] = level_arr\n",
      "    return df\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "# Load data\n",
      "binary_path = \"/Users/yk2047/Documents/GitHub/hathor/dataset_examples/halo_3517_gas.bin\"\n",
      "desc_path   = \"./plotting_codes/Example.txt\"\n",
      "desc = load_description\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Generate code with CoderAgent\n",
    "import numpy as np\n",
    "coder_instructions = np.loadtxt(DATA_DESCRIPTION_PATH, dtype=str)\n",
    "generated_code = coder_agent.generate_code(coder_instructions)\n",
    "\n",
    "print(\"=== Generated code (truncated) ===\")\n",
    "print(generated_code[:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce2b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: RUNNER AGENT (Iteration 1)\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "STEP 4: RUNNER AGENT (Iteration 2)\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "STEP 4: RUNNER AGENT (Iteration 3)\n",
      "==================================================\n",
      "\n",
      "\n",
      "ğŸ‰ SUCCESS: Code executed successfully!\n",
      "\n",
      "=== Final code after runner debugging (truncated) ===\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.patches import Ellipse\n",
      "from matplotlib.colors import LogNorm\n",
      "from scipy.io import FortranFile\n",
      "\n",
      "def load_description(desc_path):\n",
      "    import os\n",
      "    desc = {}\n",
      "    if os.path.exists(desc_path):\n",
      "        with open(desc_path, \"r\") as f:\n",
      "            for line in f:\n",
      "                line = line.strip()\n",
      "                if not line or line.startswith(\"#\"):\n",
      "                    continue\n",
      "                parts = line.split()\n",
      "                if len(parts) >= 2:\n",
      "                    col = parts[0].strip()\n",
      "                    unit = parts[-1].strip(\"[]\")\n",
      "                    desc[col] = unit\n",
      "    return desc\n",
      "\n",
      "def read_megatron_cutout_local(ff):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    header = [\n",
      "        \"redshift\",\"dx\",\"x\",\"y\",\"z\",\"vx\",\"vy\",\"vz\",\"nH\",\"T\",\"P\",\n",
      "        \"nFe\",\"nO\",\"nN\",\"nMg\",\"nNe\",\"nSi\",\"nCa\",\"nC\",\"nS\",\"nCO\",\n",
      "        \"O_I\",\"O_II\",\"O_III\",\"O_IV\",\"O_V\",\"O_VI\",\"O_VII\",\"O_VIII\",\n",
      "        \"N_I\",\"N_II\",\"N_III\",\"N_IV\",\"N_V\",\"N_VI\",\"N_VII\",\n",
      "        \"C_I\",\"C_II\",\"C_III\",\"C_IV\",\"C_V\",\"C_VI\",\n",
      "        \"Mg_I\",\"Mg_II\",\"Mg_III\",\"Mg_IV\",\"Mg_V\",\"Mg_VI\",\"Mg_VII\",\"Mg_VIII\",\"Mg_IX\",\"Mg_X\",\n",
      "        \"Si_I\",\"Si_II\",\"Si_III\",\"Si_IV\",\"Si_V\",\"Si_VI\",\"Si_VII\",\"Si_VIII\",\"Si_IX\",\"Si_X\",\"Si_XI\",\n",
      "        \"S_I\",\"S_II\",\"S_III\",\"S_IV\",\"S_V\",\"S_VI\",\"S_VII\",\"S_VIII\",\"S_IX\",\"S_X\",\"S_XI\",\n",
      "        \"Fe_I\",\"Fe_II\",\"Fe_III\",\"Fe_IV\",\"Fe_V\",\"Fe_VI\",\"Fe_VII\",\"Fe_VIII\",\"Fe_IX\",\"Fe_X\",\"Fe_XI\",\n",
      "        \"Ne_I\",\"Ne_II\",\"Ne_III\",\"Ne_IV\",\"Ne_V\",\"Ne_VI\",\"Ne_VII\",\"Ne_VIII\",\"Ne_IX\",\"Ne_X\",\n",
      "        \"H_I\",\"H_II\",\"He_II\",\"He_III\",\n",
      "        \"Habing\",\"Lyman_Werner\",\"HI_Ionising\",\"H2_Ionising\",\"HeI_Ionising\",\"HeII_ionising\"\n",
      "    ]\n",
      "    for i, _ in enumerate(header):\n",
      "        data = ff.read_reals(\"float64\")\n",
      "        if i == 0:\n",
      "            all_data = np.zeros((len(data), len(header)))\n",
      "        all_data[:, i] = data\n",
      "    df = pd.DataFrame(all_data, columns=header)\n",
      "\n",
      "    ne = (10.0**df[\"nH\"]) * df[\"H_II\"]\n",
      "    ne += ((0.24*(10.0**df[\"nH\"])/0.76)/4.0) * (df[\"He_II\"] + 2.0*df[\"He_III\"])\n",
      "    df[\"ne\"] = ne\n",
      "\n",
      "    tmp_levels = np.arange(31)\n",
      "    mpc2cm = 3.086e24\n",
      "    loc_z = df[\"redshift\"].mean()\n",
      "    levels_dx = np.log10(((20.0/0.672699966430664)/(1.0+loc_z))/(2.0**tmp_levels) * mpc2cm)\n",
      "    unique_dx = np.unique(df[\"dx\"])\n",
      "    level_arr = -999*np.ones(len(df), dtype=int)\n",
      "    for cl in unique_dx:\n",
      "        idx = np.abs(cl - levels_dx).argmin()\n",
      "        level_arr[df[\"dx\"] == cl] = tmp_levels[idx]\n",
      "    df[\"level\"] = level_arr\n",
      "    return df\n",
      "\n",
      "binary_path = \"/Users/yk2047/Documents/GitHub/hathor/dataset_examples/halo_3517_gas.bin\"\n",
      "desc_path   = \"./plotting_codes/Example.txt\"\n",
      "desc = load_description(desc_path)\n",
      "\n",
      "with FortranFile(binary_path, \"r\") as ff:\n",
      "    gas_df = read_megatron_cutout_local(ff)\n",
      "\n",
      "npix = 512\n",
      "boxsize_mpc = 20.0\n",
      "pixel_scale = boxsize_mpc / npix\n",
      "\n",
      "center = gas_df[[\"x\", \"y\", \"z\"]].median().values\n",
      "\n",
      "pos = gas_df[[\"x\", \"y\"]].values - center[:2]\n",
      "dx  = 10.0**gas_df[\"dx\"].values\n",
      "vol = dx**3\n",
      "weight = gas_df[\"ne\"].values * vol\n",
      "H, xedges, yedges = np.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 63] File name too long: 'import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.patches import Ellipse\\nfrom matplotlib.colors import LogNorm\\nfrom scipy.io import FortranFile\\n\\ndef load_description(desc_path):\\n    import os\\n    desc = {}\\n    if os.path.exists(desc_path):\\n        with open(desc_path, \"r\") as f:\\n            for line in f:\\n                line = line.strip()\\n                if not line or line.startswith(\"#\"):\\n                    continue\\n                parts = line.split()\\n                if len(parts) >= 2:\\n                    col = parts[0].strip()\\n                    unit = parts[-1].strip(\"[]\")\\n                    desc[col] = unit\\n    return desc\\n\\ndef read_megatron_cutout_local(ff):\\n    import numpy as np\\n    import pandas as pd\\n    header = [\\n        \"redshift\",\"dx\",\"x\",\"y\",\"z\",\"vx\",\"vy\",\"vz\",\"nH\",\"T\",\"P\",\\n        \"nFe\",\"nO\",\"nN\",\"nMg\",\"nNe\",\"nSi\",\"nCa\",\"nC\",\"nS\",\"nCO\",\\n        \"O_I\",\"O_II\",\"O_III\",\"O_IV\",\"O_V\",\"O_VI\",\"O_VII\",\"O_VIII\",\\n        \"N_I\",\"N_II\",\"N_III\",\"N_IV\",\"N_V\",\"N_VI\",\"N_VII\",\\n        \"C_I\",\"C_II\",\"C_III\",\"C_IV\",\"C_V\",\"C_VI\",\\n        \"Mg_I\",\"Mg_II\",\"Mg_III\",\"Mg_IV\",\"Mg_V\",\"Mg_VI\",\"Mg_VII\",\"Mg_VIII\",\"Mg_IX\",\"Mg_X\",\\n        \"Si_I\",\"Si_II\",\"Si_III\",\"Si_IV\",\"Si_V\",\"Si_VI\",\"Si_VII\",\"Si_VIII\",\"Si_IX\",\"Si_X\",\"Si_XI\",\\n        \"S_I\",\"S_II\",\"S_III\",\"S_IV\",\"S_V\",\"S_VI\",\"S_VII\",\"S_VIII\",\"S_IX\",\"S_X\",\"S_XI\",\\n        \"Fe_I\",\"Fe_II\",\"Fe_III\",\"Fe_IV\",\"Fe_V\",\"Fe_VI\",\"Fe_VII\",\"Fe_VIII\",\"Fe_IX\",\"Fe_X\",\"Fe_XI\",\\n        \"Ne_I\",\"Ne_II\",\"Ne_III\",\"Ne_IV\",\"Ne_V\",\"Ne_VI\",\"Ne_VII\",\"Ne_VIII\",\"Ne_IX\",\"Ne_X\",\\n        \"H_I\",\"H_II\",\"He_II\",\"He_III\",\\n        \"Habing\",\"Lyman_Werner\",\"HI_Ionising\",\"H2_Ionising\",\"HeI_Ionising\",\"HeII_ionising\"\\n    ]\\n    for i, _ in enumerate(header):\\n        data = ff.read_reals(\"float64\")\\n        if i == 0:\\n            all_data = np.zeros((len(data), len(header)))\\n        all_data[:, i] = data\\n    df = pd.DataFrame(all_data, columns=header)\\n\\n    ne = (10.0**df[\"nH\"]) * df[\"H_II\"]\\n    ne += ((0.24*(10.0**df[\"nH\"])'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_code[:\u001b[38;5;241m3000\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Cell 11: Save outputs to files\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# save_answer_to_file(OUTPUT_FIG_PATH, \"Figure generated by the pipeline.\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43msave_code_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./outputs/executed_codes/test.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/hathor/langchein/src/utils/file_utils.py:37\u001b[0m, in \u001b[0;36msave_code_to_file\u001b[0;34m(code, file_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_code_to_file\u001b[39m(code: \u001b[38;5;28mstr\u001b[39m, file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save Python code to file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     39\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(code)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (31 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 63] File name too long: 'import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.patches import Ellipse\\nfrom matplotlib.colors import LogNorm\\nfrom scipy.io import FortranFile\\n\\ndef load_description(desc_path):\\n    import os\\n    desc = {}\\n    if os.path.exists(desc_path):\\n        with open(desc_path, \"r\") as f:\\n            for line in f:\\n                line = line.strip()\\n                if not line or line.startswith(\"#\"):\\n                    continue\\n                parts = line.split()\\n                if len(parts) >= 2:\\n                    col = parts[0].strip()\\n                    unit = parts[-1].strip(\"[]\")\\n                    desc[col] = unit\\n    return desc\\n\\ndef read_megatron_cutout_local(ff):\\n    import numpy as np\\n    import pandas as pd\\n    header = [\\n        \"redshift\",\"dx\",\"x\",\"y\",\"z\",\"vx\",\"vy\",\"vz\",\"nH\",\"T\",\"P\",\\n        \"nFe\",\"nO\",\"nN\",\"nMg\",\"nNe\",\"nSi\",\"nCa\",\"nC\",\"nS\",\"nCO\",\\n        \"O_I\",\"O_II\",\"O_III\",\"O_IV\",\"O_V\",\"O_VI\",\"O_VII\",\"O_VIII\",\\n        \"N_I\",\"N_II\",\"N_III\",\"N_IV\",\"N_V\",\"N_VI\",\"N_VII\",\\n        \"C_I\",\"C_II\",\"C_III\",\"C_IV\",\"C_V\",\"C_VI\",\\n        \"Mg_I\",\"Mg_II\",\"Mg_III\",\"Mg_IV\",\"Mg_V\",\"Mg_VI\",\"Mg_VII\",\"Mg_VIII\",\"Mg_IX\",\"Mg_X\",\\n        \"Si_I\",\"Si_II\",\"Si_III\",\"Si_IV\",\"Si_V\",\"Si_VI\",\"Si_VII\",\"Si_VIII\",\"Si_IX\",\"Si_X\",\"Si_XI\",\\n        \"S_I\",\"S_II\",\"S_III\",\"S_IV\",\"S_V\",\"S_VI\",\"S_VII\",\"S_VIII\",\"S_IX\",\"S_X\",\"S_XI\",\\n        \"Fe_I\",\"Fe_II\",\"Fe_III\",\"Fe_IV\",\"Fe_V\",\"Fe_VI\",\"Fe_VII\",\"Fe_VIII\",\"Fe_IX\",\"Fe_X\",\"Fe_XI\",\\n        \"Ne_I\",\"Ne_II\",\"Ne_III\",\"Ne_IV\",\"Ne_V\",\"Ne_VI\",\"Ne_VII\",\"Ne_VIII\",\"Ne_IX\",\"Ne_X\",\\n        \"H_I\",\"H_II\",\"He_II\",\"He_III\",\\n        \"Habing\",\"Lyman_Werner\",\"HI_Ionising\",\"H2_Ionising\",\"HeI_Ionising\",\"HeII_ionising\"\\n    ]\\n    for i, _ in enumerate(header):\\n        data = ff.read_reals(\"float64\")\\n        if i == 0:\\n            all_data = np.zeros((len(data), len(header)))\\n        all_data[:, i] = data\\n    df = pd.DataFrame(all_data, columns=header)\\n\\n    ne = (10.0**df[\"nH\"]) * df[\"H_II\"]\\n    ne += ((0.24*(10.0**df[\"nH\"])'"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run the generated code through RunnerAgent\n",
    "\n",
    "final_code = runner_agent.run_with_debug(generated_code)\n",
    "\n",
    "print(\"\\n=== Final code after runner debugging (truncated) ===\")\n",
    "print(final_code[:3000])\n",
    "# Cell 11: Save outputs to files\n",
    "# save_answer_to_file(OUTPUT_FIG_PATH, \"Figure generated by the pipeline.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b181e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_code_to_file(final_code, \"./outputs/executed_codes/test.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Full pipeline run (same as main())\n",
    "\n",
    "pipeline = MultiAgentPipeline()\n",
    "\n",
    "pipeline.run(\n",
    "    literature_task=literature_task,\n",
    "    code_developer_prompt=code_developer_prompt,\n",
    "    num_hypotheses=3,\n",
    "    max_refinement_iterations=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Inspect output files\n",
    "\n",
    "print(\"IDEAS_DIR contents:\", os.listdir(IDEAS_DIR))\n",
    "print(\"Final code path:\", FINAL_CODE_PATH)\n",
    "print(\"Output figure path:\", OUTPUT_FIG_PATH)\n",
    "\n",
    "if os.path.exists(FINAL_CODE_PATH):\n",
    "    with open(FINAL_CODE_PATH, \"r\") as f:\n",
    "        print(\"\\n=== Final code (first 500 chars) ===\")\n",
    "        print(f.read()[:500])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hather_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
