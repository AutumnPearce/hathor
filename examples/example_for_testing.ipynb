{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1dc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for _LLMConfig\nconfig_list.1.openai.model\n  Field required [type=missing, input_value={'api_type': 'openai', 't...se_functions_api': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      7\u001b[39m access_token = get_access_token()\n\u001b[32m      8\u001b[39m config_list = [{\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-oss-120b\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# or whatever model name your endpoint expects\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: access_token,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     15\u001b[39m }]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m hathor = \u001b[43mHathor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m hathor.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hathor/src/hathor/hathor.py:25\u001b[39m, in \u001b[36mHathor.__init__\u001b[39m\u001b[34m(self, config_list, prompt, data_path, ex_code_path, interactive, literature)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.literature = literature\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_config = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig_list\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config_list,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.1\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muse_functions_api\u001b[39m\u001b[33m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     23\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_groupchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hathor/src/hathor/hathor.py:49\u001b[39m, in \u001b[36mHathor._setup_groupchat\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_groupchat\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28mself\u001b[39m.hypothesis_brainstormer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_hypoth_brainstormer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m.plot_brainstormer = \u001b[38;5;28mself\u001b[39m._create_plot_brainstormer()\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.critic = \u001b[38;5;28mself\u001b[39m._create_critic()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hathor/src/hathor/hathor.py:93\u001b[39m, in \u001b[36mHathor._create_hypoth_brainstormer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_hypoth_brainstormer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     76\u001b[39m     system_message = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m                         You are an expert in galaxy formation. You must scan through recent literature in order to create hypotheses about galaxy formation. Each hypothesis must make\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m                         only one claim that will be tested. You may additionally include your reasoning behind that claim. These hypotheses will be investigated by creating plots from R\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33m                         hypotheses, remove them and DO NOT generate new ones to take their place. Our final goal is to narrow down our options until we have one great hypothesis. \u001b[39m\n\u001b[32m     91\u001b[39m \u001b[33m                      \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResearcherAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHypothesisBrainstormer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpapers_per_query\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hathor/src/hathor/agents.py:12\u001b[39m, in \u001b[36mResearcherAgent.__init__\u001b[39m\u001b[34m(self, name, system_message, llm_config, papers_per_query)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, system_message, llm_config, papers_per_query=\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.papers = []\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m._base_system_message = system_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/autogen/agentchat/assistant_agent.py:70\u001b[39m, in \u001b[36mAssistantAgent.__init__\u001b[39m\u001b[34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, description, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     45\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     **kwargs: Any,\n\u001b[32m     53\u001b[39m ):\n\u001b[32m     54\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Args:\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    - name (str): agent name. \\n\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    - system_message (str): system message for the ChatCompletion inference. \\n\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m        [ConversableAgent](https://docs.ag2.ai/latest/docs/api-reference/autogen/ConversableAgent). \\n\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_termination_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_consecutive_auto_reply\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhuman_input_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m     81\u001b[39m         log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:262\u001b[39m, in \u001b[36mConversableAgent.__init__\u001b[39m\u001b[34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply, description, chat_messages, silent, context_variables, functions, update_agent_state_before_reply, handoffs)\u001b[39m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    258\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease implement __deepcopy__ method for each value class in llm_config to support deepcopy.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    259\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Refer to the docs for more details: https://docs.ag2.ai/docs/user-guide/advanced-concepts/llm-configuration-deep-dive/#adding-http-client-in-llm_config-for-proxy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_config = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_llm_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[38;5;28mself\u001b[39m._create_client(\u001b[38;5;28mself\u001b[39m.llm_config)\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_name(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:500\u001b[39m, in \u001b[36mConversableAgent._validate_llm_config\u001b[39m\u001b[34m(cls, llm_config)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m llm_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLLMConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/autogen/llm_config/config.py:249\u001b[39m, in \u001b[36mLLMConfig.ensure_config\u001b[39m\u001b[34m(cls, config)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mconfig_list\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:  \u001b[38;5;66;03m# backport compatibility\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLLMConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMConfig(config)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMConfig(*config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/autogen/llm_config/config.py:205\u001b[39m, in \u001b[36mLLMConfig.__init__\u001b[39m\u001b[34m(self, top_p, temperature, max_tokens, check_every_ms, allow_format_str_template, response_format, timeout, seed, cache_seed, parallel_tool_calls, tools, functions, routing_method, config_list, *configs, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    199\u001b[39m         final_config_list.append({\n\u001b[32m    200\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mapi_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# default api_type\u001b[39;00m\n\u001b[32m    201\u001b[39m             **application_level_options,\n\u001b[32m    202\u001b[39m             **c,\n\u001b[32m    203\u001b[39m         })\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43m_LLMConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mapplication_level_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_config_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_every_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_every_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_format_str_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_format_str_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouting_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouting_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/globus_env/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for _LLMConfig\nconfig_list.1.openai.model\n  Field required [type=missing, input_value={'api_type': 'openai', 't...se_functions_api': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "from hathor import Hathor\n",
    "import sys\n",
    "import os\n",
    "hathor_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, hathor_root)\n",
    "from inference_auth_token import get_access_token\n",
    "access_token = get_access_token()\n",
    "config_list = [{\n",
    "    \"model\": \"openai/gpt-oss-120b\",  # or whatever model name your endpoint expects\n",
    "    \"api_key\": access_token,\n",
    "    \"base_url\": \"https://inference-api.alcf.anl.gov/resource_server/sophia/vllm/v1\",\n",
    "    \"api_type\": \"openai\",\n",
    "    \"use_functions_api\": True,\n",
    "    \"tools\": [],\n",
    "}]\n",
    "hathor = Hathor(config_list=config_list)\n",
    "hathor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5d332",
   "metadata": {},
   "source": [
    "**Hypothesis**\n",
    "\n",
    "*“The onset of rapid, bursty star formation in low‑mass (M\\*_halo ≲ 10¹⁰ M⊙) dwarf galaxies at redshift z ≈ 2–4 is primarily regulated by the timing and strength of early, metal‑poor supernova‑driven outflows, rather than by cosmological gas accretion or re‑ionisation heating.”*\n",
    "\n",
    "**Rationale**\n",
    "\n",
    "1. **Theoretical background** – In ΛCDM, dwarf haloes grow mainly by smooth accretion and minor mergers. Classical models attribute their star‑formation histories (SFHs) to external heating (UV background) and internal feedback. Recent high‑resolution simulations suggest that *early* super‑nova (SN) explosions can evacuate gas, creating a “breathing” cycle that drives bursty SF. Distinguishing whether this internal regulation dominates over external heating is still an open question.\n",
    "\n",
    "2. **Observational motivation** – Deep HST imaging of faint dwarfs at z ≈ 2–4 shows episodic SF with duty cycles of a few hundred Myr, while their UV luminosities imply high specific SFRs. Spectroscopic metallicities are low (Z ≲ 0.1 Z⊙), consistent with metal‑poor outflows.\n",
    "\n",
    "3. **Testability with RAMSES** – The RAMSES AMR code includes:\n",
    "   - **Metal‑dependent cooling** and a **UV background** (e.g. Haardt‑Madau).\n",
    "   - **Stellar feedback** modules (thermal + kinetic SN, radiation pressure).\n",
    "   - **Tracer particles / passive scalars** for metal enrichment.\n",
    "   - **Halo catalogs** (via HOP or Rockstar) and merger trees.\n",
    "\n",
    "These ingredients allow us to isolate the impact of SN‑driven outflows by comparing runs with identical cosmology and UV background but different SN feedback prescriptions (e.g., “standard” vs. “enhanced” momentum injection).\n",
    "\n",
    "---\n",
    "\n",
    "**Plot Idea**\n",
    "\n",
    "*“Burst‑Cycle Phase Diagram: Specific Star‑Formation Rate (sSFR) vs. Central Gas‑Mass Fraction (f_gas, r < 0.1 R_vir) for dwarf haloes, colour‑coded by outflow metallicity, with over‑plotted tracks for individual haloes across cosmic time (z = 4 → 2).”*\n",
    "\n",
    "**What the Plot Shows**\n",
    "\n",
    "- **X‑axis (f_gas)** – Fraction of baryons retained in the inner 10 % of the virial radius. Low values indicate that a recent outflow has cleared the central reservoir.\n",
    "- **Y‑axis (sSFR)** – Instantaneous specific SFR (M⊙ yr⁻¹ / M\\*_star). High sSFR together with low f_gas signals a *burst* triggered by fresh inflow after an outflow.\n",
    "- **Colour (Z_out)** – Mass‑weighted metallicity of gas that has left the halo in the last 50 Myr (traced by passive scalars). Metal‑poor outflows (blue) imply early SN‑driven winds; metal‑rich outflows (red) would suggest recycling of enriched gas.\n",
    "- **Tracks** – Each dwarf halo is plotted as a line connecting its successive snapshots (Δt ≈ 20 Myr). The direction of the arrow indicates the temporal evolution.\n",
    "\n",
    "**Why This Plot Tests the Hypothesis**\n",
    "\n",
    "- If **SN‑driven outflows dominate**, we expect a *tight anti‑correlation* between f_gas and sSFR, with bursts occurring shortly after the central gas fraction recovers. The outflow metallicity should remain low (blue) during the first few bursts, rising only after several cycles of enrichment.\n",
    "- If **external heating or accretion dominates**, the relation will be weaker, and outflow metallicities will be higher (red) because the gas being expelled is already enriched by prior star formation.\n",
    "\n",
    "**Implementation Details (RAMSES‑friendly)**\n",
    "\n",
    "| Step | Action | RAMSES Feature / Tool |\n",
    "|------|--------|-----------------------|\n",
    "| 1 | **Select dwarf haloes** (M_halo ≤ 10¹⁰ M⊙) from the halo catalog at z = 4. | Rockstar/HOP output; use `pynbody` or `yt` to read. |\n",
    "| 2 | **Extract snapshots** every ~20 Myr from z = 4 to 2 (≈ 30 snapshots). | RAMSES outputs (`output_XXXXX/`). |\n",
    "| 3 | **Compute central gas mass** within 0.1 R_vir. | `yt` region selection + `sum('gas', 'cell_mass')`. |\n",
    "| 4 | **Calculate instantaneous SFR** from star‑particle formation rates over the last Δt. | Sum of `star_particle` masses formed in Δt / Δt. |\n",
    "| 5 | **Track outflowing gas** using a passive scalar that tags gas crossing a spherical shell at 0.9 R_vir with outward velocity > v_esc. Record its metallicity. | Add a scalar field in RAMSES (`outflow_tag`) and post‑process with `yt`. |\n",
    "| 6 | **Assemble the phase‑space data** for each halo and each snapshot. | Store in an HDF5 table (≈ few MB per halo). |\n",
    "| 7 | **Plot** using `matplotlib`: scatter points with arrows, colour map for metallicity, and optional density contours. | `plt.scatter`, `plt.quiver` for arrows. |\n",
    "| 8 | **Statistical test** – compute Pearson/Spearman correlation between f_gas and sSFR for each feedback model; compare distributions of Z_out. | `scipy.stats`. |\n",
    "\n",
    "**Computational Feasibility**\n",
    "\n",
    "- **Memory** – Each snapshot for a (∼ 10 Mpc) box at ∼ 10 pc resolution is ≈ 2 GB. By loading only the cells belonging to selected haloes (via a mask) the RAM usage stays < 4 GB.\n",
    "- **CPU** – Computing the central gas mass and SFR for ~200 dwarfs across 30 snapshots takes ≈ 2 h on a 6‑core laptop (using `yt` with parallelism).\n",
    "- **Disk** – The final compiled dataset (halo ID, redshift, f_gas, sSFR, Z_out) is < 10 MB, easily handled.\n",
    "\n",
    "**Potential Extensions (if time permits)**\n",
    "\n",
    "- Overlay the **UV background heating rate** (from the simulation’s cooling table) as a secondary colour axis to verify that it remains sub‑dominant.\n",
    "- Add a **control sample** of identical haloes from a run with *no* SN feedback to highlight the role of outflows.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Recommendation**\n",
    "\n",
    "Adopt the **burst‑cycle phase diagram** as the primary diagnostic plot to evaluate the hypothesis that early, metal‑poor supernova‑driven outflows are the chief regulator of bursty star formation in low‑mass dwarf galaxies at z ≈ 2–4. This approach leverages RAMSES’s strengths (AMR resolution, tracer fields, built‑in feedback modules) while remaining tractable on a research‑grade laptop.\n",
    "\n",
    "**TERMINATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d605e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22422717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    Title: Phononic Casimir Effect in Planar Materials\n",
      "                    Abstract: The Phononic Casimir effect between planar objects is investigated by deriving a formalism from the quantum partition function of the system following multiscattering approach. This fluctuation-induced coupling is mediated by phonons modeled as an effective elastic medium. We find that excitations with three types of polarizations arise from the resolved boundary conditions, however the coupling is dominated by only one of these degrees of freedom due to exponential suppression effects in the other two. The obtained scaling laws and dependence on materials properties and temperature suggest effective pathways of interaction control. Scenarios of materials combinations are envisioned where the Phononic Casimir effect is of similar order as the standard Casimir interaction mediated by electromagnetic fluctuations.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: Decision Tree Embedding by Leaf-Means\n",
      "                    Abstract: Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base\n",
      "                    Abstract: Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: Measuring and Rating Socioeconomic Disparities among Provinces: A Case of Turkiye\n",
      "                    Abstract: Regional disparities in the economic and social structures of countries have a great impact on their development levels. In geographically, culturally and economically diverse countries like Turkiye, determining the socioeconomic status of the provinces and regional differences is an important step for planning and implementing effective policies. Therefore, this study aims to determine the socioeconomic disparities of the provinces in Turkiye. For this purpose, a socioeconomic development index covering the economic and social dimensions of 81 provinces was constructed. For the index, 16 different indicators representing economic and social factors were used. These indicators were converted into indices using the Min-Max normalization method and Principal Component Analysis. Afterwards, using these indices, the provinces were divided into groups using the K-Means clustering algorithm and the Elbow method. In the last part of the study, the results are presented in a visual format using Scatter Plots, clustering maps and QGIS mapping tools. The results of the study show that 2 of the 81 provinces in Turkiye have very high, 30 high, 25 medium and 24 low socioeconomic indices. Istanbul and Ankara have very high socioeconomic status. In general, the provinces in western Turkiye have a high socioeconomic index, while the provinces in eastern and southeastern Anatolia face serious challenges in terms of socioeconomic indicators.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: On the Fourier transform of the hyperbola and its role in hyperbolic photonics\n",
      "                    Abstract: Motivated by recent breakthrough studies of wave hyperbolicity in extremely anisotropic natural materials and artificial composites, we investigate the radiation pattern of a localized emitter in a hyperbolic medium. Since the emission of a point source is associated with the Fourier transform of the iso-frequency contours of a medium, we derive and analyze the properties of the Fourier transform of hyperbolic dispersion, which sheds light into the emission properties in the presence of hyperbolic bands. Our analysis leads to a generalized form of Huygens' principle for hyperbolic waves, connecting to the emergence of negative refraction and focusing with hyperbolic media. We also highlight the occurrence of aliasing artifacts in polariton imaging. More broadly, our findings provide analytical tools to model polariton propagation in materials with extreme anisotropy, and may be applied to several other physical platforms featuring hyperbolic responses, from astrophysics to seismology.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: Azimuthal Anisotropy of Magnetic Fields in the Circumgalactic Medium Driven by Galactic Feedback Processes\n",
      "                    Abstract: We use the TNG50 cosmological magnetohydrodynamical simulation of the IllustrisTNG project to show that magnetic fields in the circumgalactic medium (CGM) have significant angular structure. This azimuthal anisotropy at fixed distance is driven by galactic feedback processes that launch strong outflows into the halo, preferentially along the minor axes of galaxies. These feedback-driven outflows entrain strong magnetic fields from the interstellar medium, dragging fields originally amplified by small-scale dynamos into the CGM. At the virial radius, $z=0$ galaxies with M$_\\star \\sim 10^{10}\\,\\rm{M_\\odot}$ show the strongest anisotropy ($\\sim 0.35$ dex). This signal weakens with decreasing impact parameter, and is also present but weaker for lower mass as well as higher mass galaxies. Creating mock Faraday rotation measure (RM) sightlines through the simulated volume, we find that the angular RM trend is qualitatively consistent with recent observational measurements. We show that rich structure is present in the circumgalactic magnetic fields of galaxies. However, TNG50 predicts small RM amplitudes in the CGM that make detection difficult as a result of other contributions along the line of sight.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: The Multi-Scale Multi-Phase Circumgalactic Medium: Observed and Simulated\n",
      "                    Abstract: These are exciting times for studies of galaxy formation and the growth of structures. New observatories and advanced simulations are revolutionising our understanding of the cycling of matter into, through, and out of galaxies. This chapter first describes why baryons are essential for galaxy evolution, providing a key test of Lambda-Cold Dark Matter cosmological model. In particular, we describe a basic framework to convert measurements of the gas properties observed in absorption spectra into global estimates of the condensed (stars and cold gas) matter mass densities. We then review our current understanding of the cycling of baryons from global to galactic scales, in the so-called circumgalactic medium. The final sections are dedicated to future prospects, identifying new techniques and up-coming facilities as well as key open questions. This chapter is complemented with a series of hands-on exercises which provide a practical guide to using publicly available hydrodynamical cosmological simulations. Beyond providing a direct connection between new observations and advanced simulations, these exercises give the reader the necessary tools to make use of these theoretical models to address their own science questions. Ultimately, our increasingly accurate description of the circumgalactic medium reveals its crucial role in transforming the pristine early Universe into the rich and diverse Universe of the present day.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: Dust in the Circumgalactic Medium of Low-Redshift Galaxies\n",
      "                    Abstract: Using spectroscopically selected galaxies from the Sloan Digital Sky Survey we present a detection of reddening due to dust in the circumgalactic medium of galaxies. We detect the mean change in the colors of \"standard crayons\" correlated with the presence of foreground galaxies at z ~0.05 as a function of angular separation. Following Peek & Graves (2010), we create standard crayons using passively evolving galaxies corrected for Milky Way reddening and color-redshift trends, leading to a sample with as little as 2% scatter in color. We devise methods to ameliorate possible systematic effects related to the estimation of colors, and we find an excess reddening induced by foreground galaxies at a level ranging from 10 to 0.5 millimagnitudes on scales ranging from 30 kpc to 1 Mpc. We attribute this effect to a large-scale distribution of dust around galaxies similar to the findings of Menard et al. 2010. We find that circumgalactic reddening is a weak function of stellar mass over the range $6 \\times 10^9 M_\\odot$ -- $6 \\times 10^{10} M_\\odot$ and note that this behavior appears to be consistent with recent results on the distribution of metals in the gas phase.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: The Influence of Environment on the Circumgalactic Medium\n",
      "                    Abstract: The effect of environment on the circumgalactic medium (CGM) is investigated through a comparison of Lya absorption line data in the Virgo Cluster and the field. This Letter uses the first systematic survey of background QSOs in and around the Virgo Cluster and large existing surveys of galaxies at low redshift. While previous studies found denser gas (higher equivalent width) closer to a galaxy (lower impact parameter), this correlation disappears in the Virgo environment. In addition, the covering fraction of the CGM is lower in the cluster environment than in the circumcluster environment and field. The results indicate that the CGM is suppressed for cluster galaxies while galaxies in the circumcluster environment have abundant CGM. The truncation of the CGM may result in the quenching of star formation through starvation. Our results also show that CGM surveys must consider the role of environment.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n",
      "\n",
      "                    Title: The Unchanging Circumgalactic Medium Over the Past 11 Billion Years\n",
      "                    Abstract: This paper examines how the circumgalactic medium (CGM) evolves as a function of time by comparing results from different absorption-line surveys that have been conducted in the vicinities of galaxies at different redshifts. Despite very different star formation properties of the galaxies considered in these separate studies and different intergalactic radiation fields at redshifts between z~2.2 and z~0, I show that both the spatial extent and mean absorption equivalent width of the CGM around galaxies of comparable mass have changed little over this cosmic time interval.\n",
      "                    Full Text: [Error extracting text: name 'requests' is not defined]\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "search =arxiv.Search(query=\"circumgalactic medium\",  \n",
    "                        max_results=20,\n",
    "                        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "                        sort_order=arxiv.SortOrder.Descending \n",
    "                    )\n",
    "results = list(client.results(search))\n",
    "\n",
    "chosen_papers = np.random.choice(results, size=5, replace=False)\n",
    "\n",
    "search =arxiv.Search(query=\"circumgalactic medium\",  \n",
    "                        max_results=20,\n",
    "                        sort_by=arxiv.SortCriterion.Relevance,\n",
    "                        sort_order=arxiv.SortOrder.Descending \n",
    "                    )\n",
    "results = list(client.results(search))\n",
    "relevant_papers = np.random.choice(results, size=5, replace=False)\n",
    "\n",
    "chosen_papers = np.concatenate([chosen_papers,relevant_papers])\n",
    "\n",
    "for paper in chosen_papers:\n",
    "    # Download and extract full text\n",
    "    try:\n",
    "        response = requests.get(paper.pdf_url)\n",
    "        pdf_file = BytesIO(response.content)\n",
    "        \n",
    "        doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        full_text = f\"[Error extracting text: {e}]\"\n",
    "    \n",
    "    # Create paper string with correct attributes\n",
    "    paper_string = f\"\"\"\n",
    "                    Title: {paper.title}\n",
    "                    Abstract: {paper.summary}\n",
    "                    Full Text: {full_text}\n",
    "                    \"\"\"\n",
    "    print(paper_string)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583b965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
